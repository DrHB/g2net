{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a93d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from scipy.stats import binned_statistic\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = '/media/iafoss/New Volume/ML/G2Net2022/data/_labels.csv'\n",
    "DATA = '/media/iafoss/New Volume/ML/G2Net2022/data/train'\n",
    "OUT = '/media/iafoss/New Volume/ML/G2Net2022/data/train.pickle'\n",
    "\n",
    "nbins=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74091639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ea288",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = list(Path(\"../data/custom_data/SIGNAL_V0/data\").glob(\"*.pth\")) + list(Path(\"../data/custom_data/SIGNAL_V1/data\").glob(\"*.pth\"))\n",
    "#df['id'] = df['id'].apply(lambda x: Path(x))\n",
    "#dataset = {}\n",
    "#for index,row in tqdm(df.iterrows(),total=len(df)):\n",
    "#    idx = row['id'].stem\n",
    "#\n",
    "#    target = row['target']\n",
    "#    data = torch.load(row['id'])\n",
    "#    time_ids = {'H1':data['H1_ts'], 'L1':data['L1_ts']}\n",
    "#    \n",
    "#    mean_statH = binned_statistic(time_ids['H1'], np.abs(data['H1_SFTs_amplitudes']*1e22)**2,\n",
    "#        statistic='mean', bins=nbins, \n",
    "#        range=(max(time_ids['H1'].min(),time_ids['L1'].min()),\n",
    "#        min(time_ids['H1'].max(),time_ids['L1'].max())))\n",
    "#    mean_statL = binned_statistic(time_ids['L1'], np.abs(data['L1_SFTs_amplitudes']*1e22)**2,\n",
    "#        statistic='mean', bins=nbins, \n",
    "#        range=(max(time_ids['H1'].min(),time_ids['L1'].min()),\n",
    "#        min(time_ids['H1'].max(),time_ids['L1'].max())))\n",
    "#    mean_statH = np.nan_to_num(np.transpose(mean_statH.statistic,(0,1)))\n",
    "#    mean_statL = np.nan_to_num(np.transpose(mean_statL.statistic,(0,1)))\n",
    "#    \n",
    "#    n_statH = binned_statistic(time_ids['H1'], np.ones((1,len(time_ids['H1']))), statistic='sum', bins=nbins, \n",
    "#        range=(max(time_ids['H1'].min(),time_ids['L1'].min()),\n",
    "#        min(time_ids['H1'].max(),time_ids['L1'].max())))\n",
    "#    n_statL = binned_statistic(time_ids['L1'], np.ones((1,len(time_ids['L1']))), statistic='sum', bins=nbins, \n",
    "#        range=(max(time_ids['H1'].min(),time_ids['L1'].min()),\n",
    "#        min(time_ids['H1'].max(),time_ids['L1'].max())))\n",
    "#    n_statH = np.nan_to_num(n_statH.statistic)[0].astype(int)\n",
    "#    n_statL = np.nan_to_num(n_statL.statistic)[0].astype(int)\n",
    "#    \n",
    "#    dataset[idx] = {'H1':mean_statH,'L1':mean_statL,'H1_ts':n_statH, 'L1_ts':n_statL}\n",
    "#\n",
    "#with open('data/gwaves_train_v5.pickle', 'wb') as handle:\n",
    "#    pickle.dump(dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca53ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gwaves(fn):\n",
    "    dataset = dict()\n",
    "    idx = fn.stem\n",
    "\n",
    "    #target = row[\"target\"]\n",
    "    data = torch.load(fn)\n",
    "    time_ids = {\"H1\": data[\"H1_ts\"], \"L1\": data[\"L1_ts\"]}\n",
    "\n",
    "    mean_statH = binned_statistic(\n",
    "        time_ids[\"H1\"],\n",
    "        np.abs(data[\"H1_SFTs_amplitudes\"] * 1e22) ** 2,\n",
    "        statistic=\"mean\",\n",
    "        bins=nbins,\n",
    "        range=(\n",
    "            max(time_ids[\"H1\"].min(), time_ids[\"L1\"].min()),\n",
    "            min(time_ids[\"H1\"].max(), time_ids[\"L1\"].max()),\n",
    "        ),\n",
    "    )\n",
    "    mean_statL = binned_statistic(\n",
    "        time_ids[\"L1\"],\n",
    "        np.abs(data[\"L1_SFTs_amplitudes\"] * 1e22) ** 2,\n",
    "        statistic=\"mean\",\n",
    "        bins=nbins,\n",
    "        range=(\n",
    "            max(time_ids[\"H1\"].min(), time_ids[\"L1\"].min()),\n",
    "            min(time_ids[\"H1\"].max(), time_ids[\"L1\"].max()),\n",
    "        ),\n",
    "    )\n",
    "    mean_statH = np.nan_to_num(np.transpose(mean_statH.statistic, (0, 1)))\n",
    "    mean_statL = np.nan_to_num(np.transpose(mean_statL.statistic, (0, 1)))\n",
    "\n",
    "    n_statH = binned_statistic(\n",
    "        time_ids[\"H1\"],\n",
    "        np.ones((1, len(time_ids[\"H1\"]))),\n",
    "        statistic=\"sum\",\n",
    "        bins=nbins,\n",
    "        range=(\n",
    "            max(time_ids[\"H1\"].min(), time_ids[\"L1\"].min()),\n",
    "            min(time_ids[\"H1\"].max(), time_ids[\"L1\"].max()),\n",
    "        ),\n",
    "    )\n",
    "    n_statL = binned_statistic(\n",
    "        time_ids[\"L1\"],\n",
    "        np.ones((1, len(time_ids[\"L1\"]))),\n",
    "        statistic=\"sum\",\n",
    "        bins=nbins,\n",
    "        range=(\n",
    "            max(time_ids[\"H1\"].min(), time_ids[\"L1\"].min()),\n",
    "            min(time_ids[\"H1\"].max(), time_ids[\"L1\"].max()),\n",
    "        ),\n",
    "    )\n",
    "    n_statH = np.nan_to_num(n_statH.statistic)[0].astype(int)\n",
    "    n_statL = np.nan_to_num(n_statL.statistic)[0].astype(int)\n",
    "\n",
    "    dataset[idx] = {\n",
    "        \"H1\": mean_statH,\n",
    "        \"L1\": mean_statL,\n",
    "        \"H1_ts\": n_statH,\n",
    "        \"L1_ts\": n_statL,\n",
    "    }\n",
    "    return dataset\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Parallel(n_jobs=16)(\n",
    "    delayed(clean_gwaves)(i)\n",
    "    for i in tqdm(signal)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c72134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import ChainMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = dict(ChainMap(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/gwaves_train_v5.pickle', 'wb') as handle:\n",
    "    pickle.dump(out, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c137d7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce7dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c9a3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98720e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_hdf5(path):\n",
    "    data = {}\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        ID_key = list(f.keys())[0]\n",
    "        # Retrieve the frequency data\n",
    "        data['freq'] = np.array(f[ID_key]['frequency_Hz'])\n",
    "        # Retrieve the Livingston decector data\n",
    "        data['L1_SFTs_amplitudes'] = np.array(f[ID_key]['L1']['SFTs'])\n",
    "        data['L1_ts'] = np.array(f[ID_key]['L1']['timestamps_GPS'])\n",
    "        # Retrieve the Hanford decector data\n",
    "        data['H1_SFTs_amplitudes'] = np.array(f[ID_key]['H1']['SFTs'])\n",
    "        data['H1_ts'] = np.array(f[ID_key]['H1']['timestamps_GPS'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_flags, std_est = {},[]\n",
    "for fname in tqdm([f for f in os.listdir('../data/test')]):\n",
    "    fidx = fname.split('.')[0]\n",
    "    data = extract_data_from_hdf5(os.path.join('../data/test',fname))\n",
    "    std_H = np.abs(data['H1_SFTs_amplitudes']*1e22).std()\n",
    "    std_L = np.abs(data['L1_SFTs_amplitudes']*1e22).std()\n",
    "    #train min,max,mean: 0.69375944,0.6990975,0.6950916\n",
    "    f0 = std_H < 0.693 or std_H > 0.700 # find nonstationary noise cases\n",
    "    f1 = std_L < 0.693 or std_L > 0.700\n",
    "    std_flags[fidx] = int(f0) + 2*int(f1)\n",
    "    if f0 == 0 and f1 == 0: continue \n",
    "    \n",
    "    time_ids = {'H1':data['H1_ts'], 'L1':data['L1_ts']}    \n",
    "    std_H = 0.5*((data['H1_SFTs_amplitudes'].real*1e22).std(0)**2 + \n",
    "             (data['H1_SFTs_amplitudes'].imag*1e22).std(0)**2)\n",
    "    std_statH = binned_statistic(time_ids['H1'], std_H,\n",
    "            statistic='mean', bins=nbins, \n",
    "            range=(max(time_ids['H1'].min(),time_ids['L1'].min()),\n",
    "            min(time_ids['H1'].max(),time_ids['L1'].max())))\n",
    "    std_statH = np.nan_to_num(std_statH.statistic**0.5,nan=1.0)\n",
    "    \n",
    "    std_L = 0.5*((data['L1_SFTs_amplitudes'].real*1e22).std(0)**2 + \n",
    "             (data['L1_SFTs_amplitudes'].imag*1e22).std(0)**2)\n",
    "    std_statL = binned_statistic(time_ids['L1'], std_L,\n",
    "            statistic='mean', bins=nbins, \n",
    "            range=(max(time_ids['H1'].min(),time_ids['L1'].min()),\n",
    "            min(time_ids['H1'].max(),time_ids['L1'].max())))\n",
    "    std_statL = np.nan_to_num(std_statL.statistic**0.5,nan=1.0)\n",
    "    \n",
    "    n_statH = binned_statistic(time_ids['H1'], np.ones((1,len(time_ids['H1']))), statistic='sum', bins=nbins, \n",
    "        range=(max(time_ids['H1'].min(),time_ids['L1'].min()),\n",
    "        min(time_ids['H1'].max(),time_ids['L1'].max())))\n",
    "    n_statL = binned_statistic(time_ids['L1'], np.ones((1,len(time_ids['L1']))), statistic='sum', bins=nbins, \n",
    "        range=(max(time_ids['H1'].min(),time_ids['L1'].min()),\n",
    "        min(time_ids['H1'].max(),time_ids['L1'].max())))\n",
    "    n_statH = np.nan_to_num(n_statH.statistic)[0].astype(int)\n",
    "    n_statL = np.nan_to_num(n_statL.statistic)[0].astype(int)\n",
    "    \n",
    "    std_est.append({'H1_std':std_statH,'L1_std':std_statL,'H1_ts':n_statH, 'L1_ts':n_statL})\n",
    "\n",
    "with open('data/real_noise_std.pickle', 'wb') as handle:\n",
    "    pickle.dump(std_est, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "df_test_std = pd.DataFrame({'id':std_flags.keys(),'std_flag':std_flags.values()})\n",
    "df_test_std.to_csv('data/test_std.csv',index=False)\n",
    "df_test_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ff386",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/real_noise_std.pickle', 'rb') as handle:\n",
    "    std_est = pickle.load(handle)\n",
    "\n",
    "i=0\n",
    "plt.plot(std_est[i]['H1_std'])\n",
    "plt.plot(std_est[i]['L1_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c012a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "paths = ['../data/train',\n",
    "         '../data/test']\n",
    "\n",
    "def get_ts(idx):\n",
    "    data = {}\n",
    "    path = os.path.join(p)\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        ID_key = list(f.keys())[0]\n",
    "        data['L1_ts'] = np.array(f[ID_key]['L1']['timestamps_GPS'])\n",
    "        data['H1_ts'] = np.array(f[ID_key]['H1']['timestamps_GPS'])\n",
    "    return data\n",
    "\n",
    "timestamps_all = []\n",
    "for path in paths:\n",
    "    for p in tqdm([os.path.join(path,p) for p in os.listdir(path)]):\n",
    "        timestamps = get_ts(p)\n",
    "        if len(timestamps['L1_ts']) < 4000 or len(timestamps['H1_ts']) < 4000: continue\n",
    "        timestamps_all.append(timestamps)\n",
    "\n",
    "with open('data/timestamps_all.pickle', 'wb') as handle:\n",
    "    pickle.dump(timestamps_all, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e042a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
