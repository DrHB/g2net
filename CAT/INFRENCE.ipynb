{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e383b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import copy\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.cuda.set_device(1)\n",
    "from pdb import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from x_transformers import  Encoder, Decoder\n",
    "from x_transformers.autoregressive_wrapper import exists\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from fastai.vision.all import BCEWithLogitsLossFlat\n",
    "from transformers.optimization import (\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "import os\n",
    "from timm import create_model\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "import pickle\n",
    "from scipy.stats import binned_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab37361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    bs = 32\n",
    "    nw = 4\n",
    "    model_name = \"convnext_large_in22k\"\n",
    "    lr = 1e-4\n",
    "    wd = 1e-4\n",
    "    epoch = 12\n",
    "    warmup_pct = 0.1\n",
    "    num_classes = 1\n",
    "    dropout_rate = 0.3\n",
    "    folder = \"EXP_200_BASELINE_CASHE_V3\"\n",
    "    mixup=False\n",
    "    exp_name = f\"{folder}_{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a785480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snr(left, right, df):\n",
    "    df_ = pd.concat([df.query(f\"snr>{left} & snr<{right}\"), df.query(\"snr==0\")])\n",
    "    return df_\n",
    "\n",
    "\n",
    "def generate_report(df):\n",
    "    val_df_eval = df.copy()\n",
    "\n",
    "    roc_100 = roc_auc_score(val_df_eval[\"target\"], val_df_eval[\"pred\"])\n",
    "\n",
    "    roc_25_50 = roc_auc_score(\n",
    "        get_snr(30, 50, val_df_eval)[\"target\"], get_snr(30, 50, val_df_eval)[\"pred\"]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"roc_all\": roc_100,\n",
    "        \"roc_30_50\": roc_25_50,\n",
    "    }\n",
    "\n",
    "def read_pkl(filename):  \n",
    "    data = dict()\n",
    "    with open(filename, 'rb') as file1: \n",
    "        k = pickle.load(file1)\n",
    "        data['L1_SFTs_amplitudes'] = np.array(k[\"L1\"]['spectrogram'])\n",
    "        data['L1_ts'] = np.array(k[\"L1\"]['timestamps'])\n",
    "        # Retrieve the Hanford decector data\n",
    "        data['H1_SFTs_amplitudes'] =  np.array(k[\"H1\"]['spectrogram'])\n",
    "        data['H1_ts'] = np.array(k[\"H1\"]['timestamps'])\n",
    "        data['freq'] = np.array(k['frequency'])\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_data(path):\n",
    "    data = {}\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        ID_key =  path.stem\n",
    "        # Retrieve the frequency data\n",
    "        try:\n",
    "            data['freq'] = np.array(f['frequency_Hz'])\n",
    "        except:\n",
    "            data['freq'] = np.array(f[ID_key]['frequency_Hz'])\n",
    "        # Retrieve the Livingston decector data\n",
    "        data['L1_SFTs_amplitudes'] = np.array(f[ID_key]['L1']['SFTs'])\n",
    "        data['L1_ts'] = np.array(f[ID_key]['L1']['timestamps_GPS'])\n",
    "        # Retrieve the Hanford decector data\n",
    "        data['H1_SFTs_amplitudes'] = np.array(f[ID_key]['H1']['SFTs'])\n",
    "        data['H1_ts'] = np.array(f[ID_key]['H1']['timestamps_GPS'])\n",
    "    return data\n",
    "    \n",
    "    \n",
    "def normalize(data, sz_t=128):     \n",
    "    time_ids = {\"H1\": data[\"H1_ts\"], \"L1\": data[\"L1_ts\"]}\n",
    "    mean_statH = binned_statistic(\n",
    "        time_ids[\"H1\"],\n",
    "        np.abs(data[\"H1_SFTs_amplitudes\"] * 1e22) ** 2,\n",
    "        statistic=\"mean\",\n",
    "        bins=sz_t,\n",
    "        range=(\n",
    "            max(time_ids[\"H1\"].min(), time_ids[\"L1\"].min()),\n",
    "            min(time_ids[\"H1\"].max(), time_ids[\"L1\"].max()),\n",
    "        ),\n",
    "    )\n",
    "    mean_statL = binned_statistic(\n",
    "        time_ids[\"L1\"],\n",
    "        np.abs(data[\"L1_SFTs_amplitudes\"] * 1e22) ** 2,\n",
    "        statistic=\"mean\",\n",
    "        bins=sz_t,\n",
    "        range=(\n",
    "            max(time_ids[\"H1\"].min(), time_ids[\"L1\"].min()),\n",
    "            min(time_ids[\"H1\"].max(), time_ids[\"L1\"].max()),\n",
    "        ),\n",
    "    )\n",
    "    mean_statH = np.nan_to_num(np.transpose(mean_statH.statistic, (0, 1)))\n",
    "    mean_statL = np.nan_to_num(np.transpose(mean_statL.statistic, (0, 1)))\n",
    "\n",
    "    x = torch.from_numpy(np.stack([mean_statH, mean_statL], 0).astype(np.float32))\n",
    "    c, h, w = x.shape\n",
    "    x[0] /= torch.max(x[0].mean(0, keepdim=True), 0.1 * torch.ones_like(x[0]))\n",
    "    x[1] /= torch.max(x[1].mean(0, keepdim=True), 0.1 * torch.ones_like(x[1]))\n",
    "    #x = torch.cat([x, 0.5 * (x[0] + x[1]).unsqueeze(0)], 0)\n",
    "    return x\n",
    "\n",
    "\n",
    "    \n",
    "def normalize_pickle(data, sz_t=128):     \n",
    "    time_ids = {\"H1\": data[\"H1_ts\"], \"L1\": data[\"L1_ts\"]}\n",
    "    mean_statH = binned_statistic(\n",
    "        time_ids[\"H1\"],\n",
    "        data[\"H1_SFTs_amplitudes\"],\n",
    "        statistic=\"mean\",\n",
    "        bins=sz_t,\n",
    "        range=(\n",
    "            max(time_ids[\"H1\"].min(), time_ids[\"L1\"].min()),\n",
    "            min(time_ids[\"H1\"].max(), time_ids[\"L1\"].max()),\n",
    "        ),\n",
    "    )\n",
    "    mean_statL = binned_statistic(\n",
    "        time_ids[\"L1\"],\n",
    "        data[\"L1_SFTs_amplitudes\"],\n",
    "        statistic=\"mean\",\n",
    "        bins=sz_t,\n",
    "        range=(\n",
    "            max(time_ids[\"H1\"].min(), time_ids[\"L1\"].min()),\n",
    "            min(time_ids[\"H1\"].max(), time_ids[\"L1\"].max()),\n",
    "        ),\n",
    "    )\n",
    "    mean_statH = np.nan_to_num(np.transpose(mean_statH.statistic, (0, 1)))\n",
    "    mean_statL = np.nan_to_num(np.transpose(mean_statL.statistic, (0, 1)))\n",
    "\n",
    "    x = torch.from_numpy(np.stack([mean_statH, mean_statL], 0).astype(np.float32))\n",
    "    c, h, w = x.shape\n",
    "    x[0] /= torch.max(x[0].mean(0, keepdim=True), 0.1 * torch.ones_like(x[0]))\n",
    "    x[1] /= torch.max(x[1].mean(0, keepdim=True), 0.1 * torch.ones_like(x[1]))\n",
    "    #x = torch.cat([x, 0.5 * (x[0] + x[1]).unsqueeze(0)], 0)\n",
    "    return x\n",
    "    \n",
    "#generating valid\n",
    "class ValLoader(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    dataset = Dataset(data_type, df)\n",
    "\n",
    "    img, y = dataset[i]\n",
    "      img (np.float32): 2 x 360 x 128\n",
    "      y (np.float32): label 0 or 1\n",
    "    \"\"\"\n",
    "    def __init__(self, df, freq_tfms=False):\n",
    "        self.df = df\n",
    "        self.tfms = freq_tfms\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        i (int): get ith data\n",
    "        \"\"\"\n",
    "        r = self.df.iloc[i]\n",
    "        y = np.float32(r.target)\n",
    "        img = normalize(read_data(r.id))\n",
    "        \n",
    "        return img, y\n",
    "    \n",
    "    \n",
    "    \n",
    "class ValLoaderPickle(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    dataset = Dataset(data_type, df)\n",
    "\n",
    "    img, y = dataset[i]\n",
    "      img (np.float32): 2 x 360 x 128\n",
    "      y (np.float32): label 0 or 1\n",
    "    \"\"\"\n",
    "    def __init__(self, df, freq_tfms=False):\n",
    "        self.df = df\n",
    "        self.tfms = freq_tfms\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        i (int): get ith data\n",
    "        \"\"\"\n",
    "        r = self.df.iloc[i]\n",
    "        y = np.float32(r.target)\n",
    "        img = normalize_pickle(read_pkl(r.id))\n",
    "        return img.float(), y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7faec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_rot90_cw(x):\n",
    "    return x.rot90(k=-1, dims=(2, 3))\n",
    "\n",
    "\n",
    "def torch_fliplr(x: Tensor):\n",
    "    \"\"\"\n",
    "    Flip 4D image tensor horizontally\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return x.flip(3)\n",
    "\n",
    "\n",
    "def torch_flipud(x: Tensor):\n",
    "    \"\"\"\n",
    "    Flip 4D image tensor vertically\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return x.flip(2)\n",
    "\n",
    "\n",
    "def tencrop_image2label(model: nn.Module, image: Tensor) -> Tensor:\n",
    "    \"\"\"Test-time augmentation for image classification that takes five crops out of input tensor (4 on corners and central)\n",
    "    and averages predictions from them and from their horisontally-flipped versions (10-Crop TTA).\n",
    "    :param model: Classification model\n",
    "    :param image: Input image tensor\n",
    "    :param crop_size: Crop size. Must be smaller than image size\n",
    "    :return: Averaged logits\n",
    "    \"\"\"\n",
    "\n",
    "    output = (\n",
    "        torch.sigmoid(model(image))\n",
    "        + torch.sigmoid(model(torch_flipud(image)))\n",
    "        + torch.sigmoid(model(torch_fliplr(image)))\n",
    "        #+ torch.sigmoid(model(torch_flipud(torch_fliplr(image))))\n",
    "        #+ torch.sigmoid(model(torch_fliplr(torch_flipud(image))))\n",
    "    ) / 3.\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b5a5a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_eval = pd.read_csv('../../val/val.csv')\n",
    "#df_eval.id = df_eval.id.apply(lambda x: Path(f\"../../val/v18val/{x}.pickle\"))\n",
    "#df_eval.columns = ['id', 'base_id', 'snr', 'target', 'f0', 'F1', 'F2', 'Alpha',\n",
    "#       'Delta', 'cosi', 'psi', 'phi', 'path', 'freq', 'nonstationary',\n",
    "#       'artifact']\n",
    "#sub_ds = ValLoaderPickle(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6b3a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub = pd.read_csv('../data/sample_submission.csv')\n",
    "#sub['id'] = sub['id'].apply(lambda x: Path(f'../data/test/{x}.hdf5'))\n",
    "#sub_ds = ValLoader(sub)\n",
    "#vld_dl = DataLoader(\n",
    "#    sub_ds,\n",
    "#    batch_size=CFG.bs,\n",
    "#    shuffle=False,\n",
    "#    num_workers=CFG.nw,\n",
    "#    pin_memory=True,\n",
    "#    drop_last=False\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b8ac135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tta(dl, model):\n",
    "    res = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(vld_dl):\n",
    "            out = tencrop_image2label(model, x.cuda()).detach().cpu()\n",
    "            #out = torch.sigmoid(model(x.cuda())).detach().cpu()\n",
    "            res.append(out)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c82214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "real_noise_fns = sorted(\n",
    "    Path(\"../data/custom_data/DATA_V33/data/\").glob(\"*.pth\"),\n",
    "    key=lambda x: str(x).split(\"_\")[-2],\n",
    ")\n",
    "\n",
    "cashe_fns = list(Path(\"cashe_dataset\").glob(\"*.pth\"))\n",
    "\n",
    "val_df = pd.read_csv(\"../data/SPLITS/V_22/val_df.csv\")\n",
    "comp_train = pd.read_csv(\"../data/train_labels.csv\")\n",
    "comp_train.columns = [\"fn\", \"target\"]\n",
    "comp_train = comp_train.query(\"target>=0\")\n",
    "comp_train[\"fn\"] = comp_train[\"fn\"].apply(lambda x: Path(\"../data/train\") / f\"{x}.hdf5\")\n",
    "comp_train.columns = [\"id\", \"target\"]\n",
    "comp_train[\"data_type\"] = \"comp_train\"\n",
    "real_noise_df = pd.DataFrame({\"id\": real_noise_fns[1100:], \"target\": 0.0, \"snr\": 0})\n",
    "real_noise_df[\"id\"] = real_noise_df[\"id\"].apply(\n",
    "    lambda x: Path(str(x).replace(\".pth\", \".h5\"))\n",
    ")\n",
    "\n",
    "val_df = pd.concat([val_df, comp_train, real_noise_df], ignore_index=True)\n",
    "val_df['id']= val_df['id'].apply(lambda x: Path(x))\n",
    "#val_df['id'] = val_df['id'].apply(lambda x: Path(f'cashe_dataset_eval/{x.stem}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6daccbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fns =  [Path('CAT_V0/CAT_V0_convnext_large_in22k_0_44.pth')] + list(Path('CAT_V2').glob(\"*.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "656c1612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.4257, 0.9237, 0.5995,  ..., 0.9436, 0.9797, 1.0478],\n",
       "         [1.0478, 0.8706, 1.3644,  ..., 0.8715, 1.0106, 0.7471],\n",
       "         [0.9354, 0.8743, 1.0378,  ..., 1.1714, 1.2512, 0.9258],\n",
       "         ...,\n",
       "         [0.9310, 1.0766, 0.8122,  ..., 0.5935, 0.8590, 1.1191],\n",
       "         [1.2538, 0.9870, 1.1841,  ..., 0.9824, 0.9042, 0.7626],\n",
       "         [1.1259, 0.8096, 0.8618,  ..., 0.9382, 0.6639, 0.8852]],\n",
       "\n",
       "        [[1.1318, 0.7519, 1.0117,  ..., 0.8561, 1.1778, 0.7206],\n",
       "         [0.9034, 1.0082, 0.9763,  ..., 1.2562, 1.3224, 0.9209],\n",
       "         [0.9683, 1.0693, 1.0762,  ..., 1.0046, 0.7339, 1.0932],\n",
       "         ...,\n",
       "         [1.0176, 1.3582, 1.1135,  ..., 1.0263, 0.8814, 1.3530],\n",
       "         [0.9604, 0.8560, 1.1153,  ..., 1.0070, 0.8463, 0.9311],\n",
       "         [1.2048, 1.0919, 0.8910,  ..., 0.9754, 0.8320, 0.9896]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6832ec7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______\n",
      "CAT_V0/CAT_V0_convnext_large_in22k_0_44.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████████████████████▏   | 140/147 [03:34<00:09,  1.32s/it]"
     ]
    }
   ],
   "source": [
    "for mw in fns:\n",
    "    print('_______')\n",
    "    print(mw)\n",
    "    #df_eval = pd.read_csv('../../val/val.csv')\n",
    "    #df_eval.id = df_eval.id.apply(lambda x: Path(f\"../../val/v18val/{x}.pickle\"))\n",
    "    #df_eval.columns = ['id', 'base_id', 'snr', 'target', 'f0', 'F1', 'F2', 'Alpha',\n",
    "    #       'Delta', 'cosi', 'psi', 'phi', 'path', 'freq', 'nonstationary',\n",
    "    #       'artifact']\n",
    "    #sub_ds = ValLoaderPickle(df_eval)\n",
    "    sub_ds = ValLoader(val_df)\n",
    "    vld_dl = DataLoader(\n",
    "        sub_ds,\n",
    "        batch_size=CFG.bs,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.nw,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    custom_model = create_model(\n",
    "                        CFG.model_name,\n",
    "                        pretrained=True,\n",
    "                        num_classes=1,\n",
    "                        in_chans=2,\n",
    "                    )\n",
    "\n",
    "    custom_model.load_state_dict(torch.load(mw))\n",
    "    custom_model.cuda();\n",
    "    custom_model.eval();\n",
    "\n",
    "    res = predict_tta(vld_dl, custom_model)\n",
    "    df_eval['pred'] = torch.cat(res).view(-1).numpy()\n",
    "    break\n",
    "    df_eval['snr'] = df_eval['snr'].replace(1000, 0)\n",
    "    df_eval = df_eval.dropna(subset='pred')\n",
    "    dict_res = generate_report(df_eval)\n",
    "    dict_res_400_500 = generate_report(df_eval.query('freq>400 and freq<500'))\n",
    "    dict_res_300_400 = generate_report(df_eval.query('freq>300 and freq<400'))\n",
    "    dict_res_200_300 = generate_report(df_eval.query('freq>200 and freq<300'))\n",
    "    dict_res_50_200 = generate_report(df_eval.query('freq>50 and freq<200'))\n",
    "    print('___all___')\n",
    "    print(dict_res)\n",
    "    print('freq_400_500:')\n",
    "    print(dict_res_400_500)\n",
    "    print('freq_300_400:')\n",
    "    print(dict_res_300_400)\n",
    "    print('freq_200_300:')\n",
    "    print(dict_res_200_300)\n",
    "    print('freq_50_200:')\n",
    "    print(dict_res_50_200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edc742fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_eval['pred'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b17cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.718026"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
