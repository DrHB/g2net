{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e383b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import copy\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pdb import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from x_transformers import  Encoder, Decoder\n",
    "from x_transformers.autoregressive_wrapper import exists\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from fastai.vision.all import BCEWithLogitsLossFlat\n",
    "from transformers.optimization import (\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "import os\n",
    "from timm import create_model\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from secrets import token_hex\n",
    "from scipy.stats import binned_statistic\n",
    "\n",
    "def get_random_name(len_k=16):\n",
    "    token = token_hex(len_k)\n",
    "    return token\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#h5 for DATA_V31, DATA_V32, train\n",
    "#everything else is .pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def read_data(path):\n",
    "    data = {}\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        ID_key =  path.stem\n",
    "        # Retrieve the frequency data\n",
    "        try:\n",
    "            data['freq'] = np.array(f['frequency_Hz'])\n",
    "        except:\n",
    "            data['freq'] = np.array(f[ID_key]['frequency_Hz'])\n",
    "        # Retrieve the Livingston decector data\n",
    "        data['L1_SFTs_amplitudes'] = np.array(f[ID_key]['L1']['SFTs'])\n",
    "        data['L1_ts'] = np.array(f[ID_key]['L1']['timestamps_GPS'])\n",
    "        # Retrieve the Hanford decector data\n",
    "        data['H1_SFTs_amplitudes'] = np.array(f[ID_key]['H1']['SFTs'])\n",
    "        data['H1_ts'] = np.array(f[ID_key]['H1']['timestamps_GPS'])\n",
    "    return data\n",
    "    \n",
    "    \n",
    "def normalize(data, sz_t=128):     \n",
    "    time_ids = {\"H1\": data[\"H1_ts\"], \"L1\": data[\"L1_ts\"]}\n",
    "    mean_statH = binned_statistic(\n",
    "        time_ids[\"H1\"],\n",
    "        np.abs(data[\"H1_SFTs_amplitudes\"] * 1e22) ** 2,\n",
    "        statistic=\"mean\",\n",
    "        bins=sz_t,\n",
    "        range=(\n",
    "            max(time_ids[\"H1\"].min(), time_ids[\"L1\"].min()),\n",
    "            min(time_ids[\"H1\"].max(), time_ids[\"L1\"].max()),\n",
    "        ),\n",
    "    )\n",
    "    mean_statL = binned_statistic(\n",
    "        time_ids[\"L1\"],\n",
    "        np.abs(data[\"L1_SFTs_amplitudes\"] * 1e22) ** 2,\n",
    "        statistic=\"mean\",\n",
    "        bins=sz_t,\n",
    "        range=(\n",
    "            max(time_ids[\"H1\"].min(), time_ids[\"L1\"].min()),\n",
    "            min(time_ids[\"H1\"].max(), time_ids[\"L1\"].max()),\n",
    "        ),\n",
    "    )\n",
    "    mean_statH = np.nan_to_num(np.transpose(mean_statH.statistic, (0, 1)))\n",
    "    mean_statL = np.nan_to_num(np.transpose(mean_statL.statistic, (0, 1)))\n",
    "\n",
    "    x = torch.from_numpy(np.stack([mean_statH, mean_statL], 0).astype(np.float32))\n",
    "    c, h, w = x.shape\n",
    "    x[0] /= torch.max(x[0].mean(0, keepdim=True), 0.1 * torch.ones_like(x[0]))\n",
    "    x[1] /= torch.max(x[1].mean(0, keepdim=True), 0.1 * torch.ones_like(x[1]))\n",
    "    #x = torch.cat([x, 0.5 * (x[0] + x[1]).unsqueeze(0)], 0)\n",
    "    return x\n",
    "    \n",
    "#generating valid\n",
    "class ValLoader(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    dataset = Dataset(data_type, df)\n",
    "\n",
    "    img, y = dataset[i]\n",
    "      img (np.float32): 2 x 360 x 128\n",
    "      y (np.float32): label 0 or 1\n",
    "    \"\"\"\n",
    "    def __init__(self, df, freq_tfms=False):\n",
    "        self.df = df\n",
    "        self.tfms = freq_tfms\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        i (int): get ith data\n",
    "        \"\"\"\n",
    "        r = self.df.iloc[i]\n",
    "        y = np.float32(r.target)\n",
    "        img = normalize(read_data(r.id))\n",
    "        data = {\"sft\": img, \"target\": y}\n",
    "        return data, f\"{Path(r.id).stem}.pth\"\n",
    "\n",
    "\n",
    "\n",
    "#val_df = pd.read_csv('../data/SPLITS/V_22/val_df.csv')\n",
    "#comp_train = pd.read_csv('../data/train_labels.csv')\n",
    "#comp_train.columns = ['fn', 'target']\n",
    "#comp_train = comp_train.query('target>=0')\n",
    "#comp_train['fn'] = comp_train['fn'].apply(lambda x: Path('../data/train')/f'{x}.hdf5')\n",
    "#comp_train.columns = ['id', 'target']\n",
    "#comp_train['data_type'] = 'comp_train'\n",
    "#real_noise_fns =  sorted(\n",
    "#            Path(\"../data/custom_data/DATA_V33/data/\").glob(\"*.pth\"),\n",
    "#            key=lambda x: str(x).split(\"_\")[-2])\n",
    "#real_noise_df = pd.DataFrame({\"id\": real_noise_fns[1100:], 'target': 0., 'snr': 0})\n",
    "#real_noise_df['id'] = real_noise_df['id'].apply(lambda x: Path(str(x).replace('.pth', '.h5')))\n",
    "#\n",
    "#val_df = pd.concat([val_df, comp_train, real_noise_df], ignore_index=True)\n",
    "#val_df['id'] = val_df['id'].apply(lambda x: Path(x))\n",
    "#vld_ds = ValLoader(val_df)\n",
    "#folder_name = Path('cashe_dataset_eval')\n",
    "#os.makedirs(folder_name, exist_ok=True)\n",
    "#for i in tqdm(range(len(vld_ds))):\n",
    "#    data, name = vld_ds[i]\n",
    "#    torch.save(data, folder_name/name)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc6b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(sig_, noise, w=1.):\n",
    "    sig = copy(sig_)\n",
    "    min_value_l= min(sig['L1_SFTs_amplitudes'].shape[1], noise['L1_SFTs_amplitudes'].shape[1])\n",
    "    min_value_h= min(sig['H1_SFTs_amplitudes'].shape[1], noise['H1_SFTs_amplitudes'].shape[1])\n",
    "    sig['L1_SFTs_amplitudes'] = w * sig['L1_SFTs_amplitudes'][:, :min_value_l] + noise['L1_SFTs_amplitudes'][:, :min_value_l]\n",
    "    sig['H1_SFTs_amplitudes'] = w * sig['H1_SFTs_amplitudes'][:, :min_value_h] + noise['H1_SFTs_amplitudes'][:, :min_value_h]\n",
    "    sig['H1_ts'] = sig['H1_ts'][:min_value_h]\n",
    "    sig['L1_ts'] = sig['L1_ts'][:min_value_l]\n",
    "    return sig\n",
    "\n",
    "\n",
    "def save_preoprocessed(save_folder, noise_fns, signal_fns):\n",
    "    noise_fn = random.choice(noise_fns)\n",
    "    signal_fn = random.choice(signal_fns)\n",
    "    \n",
    "    if np.random.rand() >= 0.66:\n",
    "        img = normalize(torch.load(noise_fn)).numpy()\n",
    "        y = 0.0\n",
    "        if np.random.rand() <= 0.5:  # horizontal flip\n",
    "            img = np.flip(img, axis=1).copy()\n",
    "        if np.random.rand() <= 0.5:  # vertical flip\n",
    "            img = np.flip(img, axis=2).copy()\n",
    "        if np.random.rand() <= 0.5:  # vertical shift\n",
    "            img = np.roll(img, np.random.randint(low=0, high=img.shape[1]), axis=1)\n",
    "        if np.random.rand() <= 0.5:  # channel shuffle\n",
    "            img = img[np.random.permutation([0, 1]), ...]\n",
    "        img = torch.tensor(img)\n",
    "        name = get_random_name() + '.pth'\n",
    "    else:\n",
    "        \n",
    "        img = normalize(\n",
    "            combine(torch.load(signal_fn), torch.load(noise_fn))\n",
    "        )\n",
    "        y = 1.0\n",
    "        name = noise_fn.stem + '_' + signal_fn.stem + '.pth'\n",
    "    data = {\"sft\": img, \"target\": y}\n",
    "   \n",
    "    return torch.save(data, save_folder/name)\n",
    "\n",
    "signal = list(Path(\"../data/custom_data/SIGNAL_V0/data\").glob(\"*.pth\")) + list(Path(\"../data/custom_data/SIGNAL_V1/data\").glob(\"*.pth\"))\n",
    "\n",
    "real_noise_fns = sorted(\n",
    "    Path(\"../data/custom_data/DATA_V33/data/\").glob(\"*.pth\"),\n",
    "    key=lambda x: str(x).split(\"_\")[-2],\n",
    ")\n",
    "\n",
    "fake_noise_fns = sorted(\n",
    "    Path(\"../data/custom_data/DATA_V34/data/\").glob(\"*.pth\"),\n",
    "    key=lambda x: str(x).split(\"_\")[-2],\n",
    ")\n",
    "\n",
    "noise_sim = list(Path(\"../data/custom_data/DATA_V31_V32_NOISE/\").glob(\"*.pth\"))\n",
    "\n",
    "noise = real_noise_fns[:1100] + fake_noise_fns  + noise_sim\n",
    "\n",
    "\n",
    "len(signal), len(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = Path('cashe_dataset')\n",
    "n_samples = 20000\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "#for i in tqdm(range(1000)):\n",
    "#    save_preoprocessed(folder_name, noise, signal)\n",
    "#\n",
    "Parallel(n_jobs=16)(\n",
    "    delayed(save_preoprocessed)(save_folder=folder_name, noise_fns=noise, signal_fns=signal)\n",
    "    for i in tqdm(range(n_samples))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83526e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def cehck_torch(x):\n",
    "#    try:\n",
    "#        torch.load(x)\n",
    "#        return 0\n",
    "#    except:\n",
    "#        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768923c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in tqdm(signal):\n",
    "#    try:\n",
    "#        torch.load(i)\n",
    "#    except:\n",
    "#        print(i)\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e661b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = Parallel(n_jobs=16)(\n",
    "#    delayed(cehck_torch)(i)\n",
    "#    for i in tqdm(signal)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e826f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
