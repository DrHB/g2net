{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea0d0b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import copy\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pdb import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from x_transformers import  Encoder, Decoder\n",
    "from x_transformers.autoregressive_wrapper import exists\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from fastai.vision.all import BCEWithLogitsLossFlat\n",
    "from transformers.optimization import (\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "import os\n",
    "from timm import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8dc8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    bs = 128\n",
    "    nw = 4\n",
    "    model_name = \"VIT\"\n",
    "    lr = 1e-4\n",
    "    wd = 1e-4\n",
    "    epoch = 50\n",
    "    warmup_pct = 0.1\n",
    "    num_classes = 1\n",
    "    dropout_rate = 0.3\n",
    "    folder = \"EXP_120_00_VIT_36\"\n",
    "    mixup=False\n",
    "    exp_name = f\"{folder}_{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53f1cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snr(left, right, df):\n",
    "    df_ = pd.concat([df.query(f\"snr>{left} & snr<{right}\"), df.query(\"snr==0\")])\n",
    "    return df_\n",
    "\n",
    "\n",
    "def generate_report(df, p, fn):\n",
    "    pred = torch.sigmoid(p).cpu().numpy().reshape(-1)\n",
    "    val_df_eval = df.copy()\n",
    "    val_df_eval[\"pred\"] = pred\n",
    "    val_df_eval.to_csv(f\"{fn}_oof.csv\")\n",
    "\n",
    "    roc_100 = roc_auc_score(val_df_eval[\"target\"], val_df_eval[\"pred\"])\n",
    "    roc_0_50 = roc_auc_score(\n",
    "        get_snr(0, 50, val_df_eval)[\"target\"], get_snr(0, 50, val_df_eval)[\"pred\"]\n",
    "    )\n",
    "    roc_15_50 = roc_auc_score(\n",
    "        get_snr(15, 50, val_df_eval)[\"target\"], get_snr(15, 50, val_df_eval)[\"pred\"]\n",
    "    )\n",
    "    roc_25_50 = roc_auc_score(\n",
    "        get_snr(25, 50, val_df_eval)[\"target\"], get_snr(25, 50, val_df_eval)[\"pred\"]\n",
    "    )\n",
    "    roc_0_40 = roc_auc_score(\n",
    "        get_snr(0, 40, val_df_eval)[\"target\"], get_snr(0, 40, val_df_eval)[\"pred\"]\n",
    "    )\n",
    "\n",
    "    roc_0_30 = roc_auc_score(\n",
    "        get_snr(0, 30, val_df_eval)[\"target\"], get_snr(0, 30, val_df_eval)[\"pred\"]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"roc_all\": roc_100,\n",
    "        \"roc_0_50\": roc_0_50,\n",
    "        \"roc_15_50\": roc_15_50,\n",
    "        \"roc_25_50\": roc_25_50,\n",
    "        \"roc_0_40\": roc_0_40,\n",
    "        \"roc_0_30\": roc_0_30,\n",
    "    }\n",
    "\n",
    "class SaveModel:\n",
    "    def __init__(self, folder, exp_name, best=np.inf):\n",
    "        self.best = best\n",
    "        self.folder = Path(folder) / f\"{exp_name}.pth\"\n",
    "\n",
    "    def __call__(self, score, model, epoch):\n",
    "        if score < self.best:\n",
    "            self.best = score\n",
    "            print(f\"Better model found at epoch {epoch} with value: {self.best}.\")\n",
    "            torch.save(model.state_dict(), self.folder)\n",
    "\n",
    "\n",
    "class SaveModelMetric:\n",
    "    def __init__(self, folder, exp_name, best=-np.inf):\n",
    "        self.best = best\n",
    "        self.folder = Path(folder) / f\"{exp_name}.pth\"\n",
    "\n",
    "    def __call__(self, score, model, epoch):\n",
    "        if score > self.best:\n",
    "            self.best = score\n",
    "            print(f\"Better model found at epoch {epoch} with value: {self.best}.\")\n",
    "            torch.save(model.state_dict(), self.folder)\n",
    "\n",
    "\n",
    "class SaveModelEpoch:\n",
    "    def __init__(self, folder, exp_name, best=-np.inf):\n",
    "        self.best = best\n",
    "        self.folder = Path(folder)\n",
    "        self.exp_name = exp_name\n",
    "\n",
    "    def __call__(self, score, model, epoch):\n",
    "        self.best = score\n",
    "        print(f\"Better model found at epoch {epoch} with value: {self.best}.\")\n",
    "        torch.save(model.state_dict(), f\"{self.folder/self.exp_name}_{epoch}.pth\")\n",
    "\n",
    "\n",
    "def custom_auc_score(p, gt):\n",
    "    return roc_auc_score(gt.cpu().numpy(),  torch.sigmoid(p).cpu().numpy().reshape(-1))\n",
    "\n",
    "\n",
    "def fit_mixup(\n",
    "    epochs,\n",
    "    model,\n",
    "    train_dl,\n",
    "    valid_dl,\n",
    "    loss_fn,\n",
    "    opt,\n",
    "    metric,\n",
    "    val_df,\n",
    "    folder=\"models\",\n",
    "    exp_name=\"exp_00\",\n",
    "    device=None,\n",
    "    sched=None,\n",
    "    mixup_=False,\n",
    "    save_md=SaveModel,\n",
    "):\n",
    "    if device is None:\n",
    "        device = (\n",
    "            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        )\n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    loss_fn_trn = loss_fn\n",
    "    if mixup_:\n",
    "        mixup = Mixup(num_classes=2, mixup_alpha=0.4, prob=0.8)\n",
    "        loss_fn_trn = BinaryCrossEntropy()\n",
    "    mb = master_bar(range(epochs))\n",
    "\n",
    "    mb.write(\n",
    "        [\n",
    "            \"epoch\",\n",
    "            \"train_loss\",\n",
    "            \"valid_loss\",\n",
    "            \"val_metric\",\n",
    "            \"roc_all\",\n",
    "            \"roc_0_50\",\n",
    "            \"roc_15_50\",\n",
    "            \"roc_25_50\",\n",
    "            \"roc_0_40\",\n",
    "            \"roc_0_30\",\n",
    "        ],\n",
    "        table=True,\n",
    "    )\n",
    "    model.to(device)  # we have to put our model on gpu\n",
    "    #scaler = torch.cuda.amp.GradScaler()  # this for half precision training\n",
    "    save_md = save_md(folder, exp_name)\n",
    "\n",
    "    for i in mb:  # iterating  epoch\n",
    "        trn_loss, val_loss = 0.0, 0.0\n",
    "        trn_n, val_n = len(train_dl.dataset), len(valid_dl.dataset)\n",
    "        model.train()  # set model for training\n",
    "        for (xb, yb) in progress_bar(train_dl, parent=mb):\n",
    "            xb, yb = xb.to(device), yb.to(device)  # putting batches to device\n",
    "            if mixup_:\n",
    "                xb, yb = mixup(xb, yb)\n",
    "           \n",
    "            out = model(xb)  # forward pass\n",
    "            loss = loss_fn_trn(out, yb)  # calulation loss\n",
    "\n",
    "            trn_loss += loss.item()\n",
    "            #print(loss.item())\n",
    "            opt.zero_grad()  # zeroing optimizer\n",
    "            loss.backward()  # backward\n",
    "            opt.step()  # optimzers step\n",
    "            if sched is not None:\n",
    "                sched.step()  # scuedular step\n",
    "\n",
    "        trn_loss /= mb.child.total\n",
    "\n",
    "        # putting model in eval mode\n",
    "        model.eval()\n",
    "        gt = []\n",
    "        pred = []\n",
    "        # after epooch is done we can run a validation dataloder and see how are doing\n",
    "        with torch.no_grad():\n",
    "            for (xb, yb) in progress_bar(valid_dl, parent=mb):\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                gt.append(yb.detach())\n",
    "                pred.append(out.detach())\n",
    "        # calculating metric\n",
    "        metric_ = metric(torch.cat(pred), torch.cat(gt))\n",
    "        # saving model if necessary\n",
    "        save_md(metric_, model, i)\n",
    "        val_loss /= mb.child.total\n",
    "        dict_res = generate_report(val_df, torch.cat(pred), f\"{folder}/{exp_name}_{i}\")\n",
    "\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"trn_loss\": [trn_loss],\n",
    "                \"val_loss\": [val_loss],\n",
    "                \"metric\": [metric_],\n",
    "                \"roc_all\": [dict_res[\"roc_all\"]],\n",
    "                \"roc_0_50\": [dict_res[\"roc_0_50\"]],\n",
    "                \"roc_15_50\": [dict_res[\"roc_15_50\"]],\n",
    "                \"roc_25_50\": [dict_res[\"roc_25_50\"]],\n",
    "                \"roc_0_40\": [dict_res[\"roc_0_40\"]],\n",
    "                \"roc_0_30\": [dict_res[\"roc_0_30\"]],\n",
    "            }\n",
    "        ).to_csv(f\"{folder}/{exp_name}_{i}.csv\", index=False)\n",
    "        mb.write(\n",
    "            [\n",
    "                i,\n",
    "                f\"{trn_loss:.6f}\",\n",
    "                f\"{val_loss:.6f}\",\n",
    "                f\"{metric_:.6f}\",\n",
    "                f\"{dict_res['roc_all']:.6f}\",\n",
    "                f\"{dict_res['roc_0_50']:.6f}\",\n",
    "                f\"{dict_res['roc_15_50']:.6f}\",\n",
    "                f\"{dict_res['roc_25_50']:.6f}\",\n",
    "                f\"{dict_res['roc_0_40']:.6f}\",\n",
    "                f\"{dict_res['roc_0_30']:.6f}\",\n",
    "            ],\n",
    "            table=True,\n",
    "        )\n",
    "    print(\"Training done\")\n",
    "    # loading the best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9838c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataV0():\n",
    "    \"\"\"\n",
    "    dataset = Dataset(data_type, df)\n",
    "\n",
    "    img, y = dataset[i]\n",
    "      img (np.float32): 2 x 360 x 128\n",
    "      y (np.float32): label 0 or 1\n",
    "    \"\"\"\n",
    "    def __init__(self, df, freq_tfms=False):\n",
    "        self.df = df\n",
    "        self.tfms = freq_tfms\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        i (int): get ith data\n",
    "        \"\"\"\n",
    "        r = self.df.iloc[i]\n",
    "        y = np.float32(r.target)\n",
    "        img = np.array(torch.load(r.id)['s_p_n'])\n",
    "#        filename=r.id\n",
    "#        file_id = Path(r.id).stem\n",
    "#        img = np.empty((2, 360, 128), dtype=np.float32)\n",
    "#        with h5py.File(filename, 'r') as f:\n",
    "#            g = f[file_id]\n",
    "#            try:\n",
    "#                freq = f[\"frequency_Hz\"][1:]\n",
    "#            except:\n",
    "#                freq = g[\"frequency_Hz\"][:]\n",
    "#            for ch, s in enumerate(['H1', 'L1']):\n",
    "#                a = g[s]['SFTs'][:, :4096] * 1e22  # Fourier coefficient complex64\n",
    "#                p = a.real**2 + a.imag**2  # power\n",
    "#                p/= p.mean()\n",
    "#                p = np.mean(p.reshape(360, 128, 32), axis=2)  # compress 4096 -> 128\n",
    "#                img[ch] = p #normalize(p, clip=True)\n",
    "#        img = img - img.mean()\n",
    "#        img = img / img.std()\n",
    "        \n",
    "        if self.tfms:\n",
    "            if np.random.rand() <= 0.5:  # horizontal flip\n",
    "                img = np.flip(img, axis=1).copy()\n",
    "            if np.random.rand() <= 0.5:  # vertical flip\n",
    "                img = np.flip(img, axis=2).copy()\n",
    "            if np.random.rand() <= 0.5:  # vertical shift\n",
    "                img = np.roll(img, np.random.randint(low=0, high=img.shape[1]), axis=1)\n",
    "        return torch.tensor(img), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49267c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df = pd.read_csv('../data/SPLITS/V_20/trn_df.csv')\n",
    "trn_df['id'] = trn_df['id'].apply(lambda x: Path(x.replace('.h5', '.pth')))\n",
    "val_df = pd.read_csv('../data/SPLITS/V_20/val_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a20e529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s_p_n': tensor([[[-1.3145, -0.9405, -1.1675,  ..., -0.7726, -0.2216,  0.9371],\n",
       "          [ 0.0865, -0.9202,  0.7688,  ..., -0.7958,  0.5009,  0.8917],\n",
       "          [-0.2969, -0.1678, -0.8466,  ..., -1.3051,  0.1425,  0.5670],\n",
       "          ...,\n",
       "          [-0.3614,  0.1445, -0.1689,  ..., -0.4277, -0.3333,  0.8875],\n",
       "          [ 1.0967,  0.2559,  0.2302,  ..., -1.1830, -0.7766,  0.2369],\n",
       "          [-1.0134,  0.3110,  1.2747,  ..., -0.5835, -1.7268, -0.0904]],\n",
       " \n",
       "         [[ 1.1191,  0.5861,  0.4250,  ..., -0.8543,  1.4359, -0.6437],\n",
       "          [-0.0519, -0.1483,  0.1070,  ..., -0.0138, -1.0058, -0.7802],\n",
       "          [ 0.6024,  3.6046,  1.8719,  ..., -1.6742, -1.2916, -1.2031],\n",
       "          ...,\n",
       "          [ 0.1899,  0.4270,  0.4975,  ...,  1.6455, -0.5919,  2.5433],\n",
       "          [ 1.8684,  1.3297, -0.1776,  ..., -1.1528,  0.1790, -0.1275],\n",
       "          [ 0.5175,  0.9300, -1.1015,  ..., -0.3297, -1.1373, -0.3492]]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(trn_df['id'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba11500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_size = 360\n",
    "#patch_size = 36\n",
    "#channels = 2\n",
    "#num_patches = image_size // patch_size\n",
    "#patch_dim = channels * patch_size ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4d2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = rearrange(\n",
    "#    img.unsqueeze(0), \"b c (h p1) (w p2) -> b (h w) c p1 p2\", p1=num_patches, p2=128\n",
    "#)[0]\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57c3924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTransformerWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        attn_layers,\n",
    "        channels=2,\n",
    "        num_classes=1,\n",
    "        dropout=0.0,\n",
    "        post_emb_norm=False,\n",
    "        emb_dropout=0,\n",
    "        lenth=128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert isinstance(attn_layers, Encoder), \"attention layers must be an Encoder\"\n",
    "        dim = attn_layers.dim\n",
    "        num_patches = image_size // patch_size\n",
    "        self.patch_size = (num_patches, lenth)\n",
    "        patch_dim = self.patch_size[0] * self.patch_size[1] * channels\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, patch_size + 1, dim))\n",
    "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.post_emb_norm = nn.LayerNorm(dim) if post_emb_norm else nn.Identity()\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.attn_layers = attn_layers\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.mlp_head = (\n",
    "            nn.Linear(dim, num_classes) if exists(num_classes) else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, return_embeddings=False):\n",
    "        p = self.patch_size\n",
    "\n",
    "        x = rearrange(img, \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=p[0], p2=p[1])\n",
    "        x = self.patch_to_embedding(x)\n",
    "        n = x.shape[1]\n",
    "\n",
    "        x = x + self.pos_embedding[:, :n]\n",
    "\n",
    "        x = self.post_emb_norm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.attn_layers(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if not exists(self.mlp_head) or return_embeddings:\n",
    "            return x\n",
    "\n",
    "        x = x.mean(dim=-2)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75d5007d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>roc_all</th>\n",
       "      <th>roc_0_50</th>\n",
       "      <th>roc_15_50</th>\n",
       "      <th>roc_25_50</th>\n",
       "      <th>roc_0_40</th>\n",
       "      <th>roc_0_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.708315</td>\n",
       "      <td>0.703645</td>\n",
       "      <td>0.468591</td>\n",
       "      <td>0.468591</td>\n",
       "      <td>0.456939</td>\n",
       "      <td>0.456939</td>\n",
       "      <td>0.458442</td>\n",
       "      <td>0.456598</td>\n",
       "      <td>0.440107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.706639</td>\n",
       "      <td>0.694749</td>\n",
       "      <td>0.474931</td>\n",
       "      <td>0.474931</td>\n",
       "      <td>0.469192</td>\n",
       "      <td>0.469192</td>\n",
       "      <td>0.468743</td>\n",
       "      <td>0.474946</td>\n",
       "      <td>0.471516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.697335</td>\n",
       "      <td>0.696737</td>\n",
       "      <td>0.478410</td>\n",
       "      <td>0.478410</td>\n",
       "      <td>0.457346</td>\n",
       "      <td>0.457346</td>\n",
       "      <td>0.456771</td>\n",
       "      <td>0.449046</td>\n",
       "      <td>0.484228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.696033</td>\n",
       "      <td>0.695278</td>\n",
       "      <td>0.475707</td>\n",
       "      <td>0.475707</td>\n",
       "      <td>0.458671</td>\n",
       "      <td>0.458671</td>\n",
       "      <td>0.456361</td>\n",
       "      <td>0.470449</td>\n",
       "      <td>0.468646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.695712</td>\n",
       "      <td>0.695480</td>\n",
       "      <td>0.486224</td>\n",
       "      <td>0.486224</td>\n",
       "      <td>0.467480</td>\n",
       "      <td>0.467480</td>\n",
       "      <td>0.469812</td>\n",
       "      <td>0.460508</td>\n",
       "      <td>0.478716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.694333</td>\n",
       "      <td>0.691235</td>\n",
       "      <td>0.582160</td>\n",
       "      <td>0.582160</td>\n",
       "      <td>0.517488</td>\n",
       "      <td>0.517488</td>\n",
       "      <td>0.519064</td>\n",
       "      <td>0.521713</td>\n",
       "      <td>0.498588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.669491</td>\n",
       "      <td>0.693767</td>\n",
       "      <td>0.628319</td>\n",
       "      <td>0.628319</td>\n",
       "      <td>0.520741</td>\n",
       "      <td>0.520741</td>\n",
       "      <td>0.526945</td>\n",
       "      <td>0.498919</td>\n",
       "      <td>0.485267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.579441</td>\n",
       "      <td>0.615685</td>\n",
       "      <td>0.681927</td>\n",
       "      <td>0.681927</td>\n",
       "      <td>0.547768</td>\n",
       "      <td>0.547768</td>\n",
       "      <td>0.550106</td>\n",
       "      <td>0.509319</td>\n",
       "      <td>0.509251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.537481</td>\n",
       "      <td>0.596045</td>\n",
       "      <td>0.738616</td>\n",
       "      <td>0.738616</td>\n",
       "      <td>0.591657</td>\n",
       "      <td>0.591657</td>\n",
       "      <td>0.601900</td>\n",
       "      <td>0.543674</td>\n",
       "      <td>0.510827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.515147</td>\n",
       "      <td>0.563980</td>\n",
       "      <td>0.752278</td>\n",
       "      <td>0.752278</td>\n",
       "      <td>0.612063</td>\n",
       "      <td>0.612063</td>\n",
       "      <td>0.616671</td>\n",
       "      <td>0.569594</td>\n",
       "      <td>0.547103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.498117</td>\n",
       "      <td>0.576736</td>\n",
       "      <td>0.757961</td>\n",
       "      <td>0.757961</td>\n",
       "      <td>0.607733</td>\n",
       "      <td>0.607733</td>\n",
       "      <td>0.612654</td>\n",
       "      <td>0.559136</td>\n",
       "      <td>0.521716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.485162</td>\n",
       "      <td>0.546271</td>\n",
       "      <td>0.772053</td>\n",
       "      <td>0.772053</td>\n",
       "      <td>0.620920</td>\n",
       "      <td>0.620920</td>\n",
       "      <td>0.628692</td>\n",
       "      <td>0.563399</td>\n",
       "      <td>0.552988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.474102</td>\n",
       "      <td>0.538344</td>\n",
       "      <td>0.782458</td>\n",
       "      <td>0.782458</td>\n",
       "      <td>0.633488</td>\n",
       "      <td>0.633488</td>\n",
       "      <td>0.642636</td>\n",
       "      <td>0.566391</td>\n",
       "      <td>0.541218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.467556</td>\n",
       "      <td>0.544655</td>\n",
       "      <td>0.793446</td>\n",
       "      <td>0.793446</td>\n",
       "      <td>0.645405</td>\n",
       "      <td>0.645405</td>\n",
       "      <td>0.652791</td>\n",
       "      <td>0.585099</td>\n",
       "      <td>0.563082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.461535</td>\n",
       "      <td>0.548037</td>\n",
       "      <td>0.784176</td>\n",
       "      <td>0.784176</td>\n",
       "      <td>0.623713</td>\n",
       "      <td>0.623713</td>\n",
       "      <td>0.629269</td>\n",
       "      <td>0.561586</td>\n",
       "      <td>0.540160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.456206</td>\n",
       "      <td>0.523105</td>\n",
       "      <td>0.799044</td>\n",
       "      <td>0.799044</td>\n",
       "      <td>0.652387</td>\n",
       "      <td>0.652387</td>\n",
       "      <td>0.660897</td>\n",
       "      <td>0.585418</td>\n",
       "      <td>0.554519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.449607</td>\n",
       "      <td>0.544660</td>\n",
       "      <td>0.794848</td>\n",
       "      <td>0.794848</td>\n",
       "      <td>0.637577</td>\n",
       "      <td>0.637577</td>\n",
       "      <td>0.644862</td>\n",
       "      <td>0.572511</td>\n",
       "      <td>0.535080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.441590</td>\n",
       "      <td>0.539848</td>\n",
       "      <td>0.797898</td>\n",
       "      <td>0.797898</td>\n",
       "      <td>0.648390</td>\n",
       "      <td>0.648390</td>\n",
       "      <td>0.656266</td>\n",
       "      <td>0.581192</td>\n",
       "      <td>0.541251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.435853</td>\n",
       "      <td>0.544887</td>\n",
       "      <td>0.792285</td>\n",
       "      <td>0.792285</td>\n",
       "      <td>0.639786</td>\n",
       "      <td>0.639786</td>\n",
       "      <td>0.647958</td>\n",
       "      <td>0.575886</td>\n",
       "      <td>0.551008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.432890</td>\n",
       "      <td>0.539411</td>\n",
       "      <td>0.798404</td>\n",
       "      <td>0.798404</td>\n",
       "      <td>0.643506</td>\n",
       "      <td>0.643506</td>\n",
       "      <td>0.651851</td>\n",
       "      <td>0.575475</td>\n",
       "      <td>0.542255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.432127</td>\n",
       "      <td>0.534985</td>\n",
       "      <td>0.796093</td>\n",
       "      <td>0.796093</td>\n",
       "      <td>0.648574</td>\n",
       "      <td>0.648574</td>\n",
       "      <td>0.658916</td>\n",
       "      <td>0.581878</td>\n",
       "      <td>0.527959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.428699</td>\n",
       "      <td>0.530456</td>\n",
       "      <td>0.796261</td>\n",
       "      <td>0.796261</td>\n",
       "      <td>0.639630</td>\n",
       "      <td>0.639630</td>\n",
       "      <td>0.649936</td>\n",
       "      <td>0.570849</td>\n",
       "      <td>0.527947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.425780</td>\n",
       "      <td>0.540330</td>\n",
       "      <td>0.800381</td>\n",
       "      <td>0.800381</td>\n",
       "      <td>0.653920</td>\n",
       "      <td>0.653920</td>\n",
       "      <td>0.662160</td>\n",
       "      <td>0.593780</td>\n",
       "      <td>0.548856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.419210</td>\n",
       "      <td>0.532898</td>\n",
       "      <td>0.801063</td>\n",
       "      <td>0.801063</td>\n",
       "      <td>0.654689</td>\n",
       "      <td>0.654689</td>\n",
       "      <td>0.665491</td>\n",
       "      <td>0.593359</td>\n",
       "      <td>0.527169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.417775</td>\n",
       "      <td>0.566387</td>\n",
       "      <td>0.788292</td>\n",
       "      <td>0.788292</td>\n",
       "      <td>0.630074</td>\n",
       "      <td>0.630074</td>\n",
       "      <td>0.641693</td>\n",
       "      <td>0.561749</td>\n",
       "      <td>0.508235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.415861</td>\n",
       "      <td>0.536915</td>\n",
       "      <td>0.802647</td>\n",
       "      <td>0.802647</td>\n",
       "      <td>0.656766</td>\n",
       "      <td>0.656766</td>\n",
       "      <td>0.669964</td>\n",
       "      <td>0.590875</td>\n",
       "      <td>0.523374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.410085</td>\n",
       "      <td>0.544952</td>\n",
       "      <td>0.801057</td>\n",
       "      <td>0.801057</td>\n",
       "      <td>0.649602</td>\n",
       "      <td>0.649602</td>\n",
       "      <td>0.659543</td>\n",
       "      <td>0.588289</td>\n",
       "      <td>0.541222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.411784</td>\n",
       "      <td>0.536116</td>\n",
       "      <td>0.796495</td>\n",
       "      <td>0.796495</td>\n",
       "      <td>0.639849</td>\n",
       "      <td>0.639849</td>\n",
       "      <td>0.649918</td>\n",
       "      <td>0.571710</td>\n",
       "      <td>0.531798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.404409</td>\n",
       "      <td>0.554625</td>\n",
       "      <td>0.797785</td>\n",
       "      <td>0.797785</td>\n",
       "      <td>0.646064</td>\n",
       "      <td>0.646064</td>\n",
       "      <td>0.657748</td>\n",
       "      <td>0.581729</td>\n",
       "      <td>0.524325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.400530</td>\n",
       "      <td>0.540463</td>\n",
       "      <td>0.796338</td>\n",
       "      <td>0.796338</td>\n",
       "      <td>0.641579</td>\n",
       "      <td>0.641579</td>\n",
       "      <td>0.654221</td>\n",
       "      <td>0.576737</td>\n",
       "      <td>0.515494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.398780</td>\n",
       "      <td>0.556132</td>\n",
       "      <td>0.796112</td>\n",
       "      <td>0.796112</td>\n",
       "      <td>0.638688</td>\n",
       "      <td>0.638688</td>\n",
       "      <td>0.649535</td>\n",
       "      <td>0.573854</td>\n",
       "      <td>0.518926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.392871</td>\n",
       "      <td>0.569957</td>\n",
       "      <td>0.795587</td>\n",
       "      <td>0.795587</td>\n",
       "      <td>0.639993</td>\n",
       "      <td>0.639993</td>\n",
       "      <td>0.649867</td>\n",
       "      <td>0.572478</td>\n",
       "      <td>0.527309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.393228</td>\n",
       "      <td>0.554398</td>\n",
       "      <td>0.802146</td>\n",
       "      <td>0.802146</td>\n",
       "      <td>0.650639</td>\n",
       "      <td>0.650639</td>\n",
       "      <td>0.660451</td>\n",
       "      <td>0.583713</td>\n",
       "      <td>0.531066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.393397</td>\n",
       "      <td>0.553764</td>\n",
       "      <td>0.797783</td>\n",
       "      <td>0.797783</td>\n",
       "      <td>0.645716</td>\n",
       "      <td>0.645716</td>\n",
       "      <td>0.657531</td>\n",
       "      <td>0.585620</td>\n",
       "      <td>0.525733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.385163</td>\n",
       "      <td>0.564115</td>\n",
       "      <td>0.802104</td>\n",
       "      <td>0.802104</td>\n",
       "      <td>0.649605</td>\n",
       "      <td>0.649605</td>\n",
       "      <td>0.660679</td>\n",
       "      <td>0.591043</td>\n",
       "      <td>0.536782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.383482</td>\n",
       "      <td>0.601905</td>\n",
       "      <td>0.798895</td>\n",
       "      <td>0.798895</td>\n",
       "      <td>0.644659</td>\n",
       "      <td>0.644659</td>\n",
       "      <td>0.654497</td>\n",
       "      <td>0.582194</td>\n",
       "      <td>0.535844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.384214</td>\n",
       "      <td>0.567962</td>\n",
       "      <td>0.797358</td>\n",
       "      <td>0.797358</td>\n",
       "      <td>0.641985</td>\n",
       "      <td>0.641985</td>\n",
       "      <td>0.652908</td>\n",
       "      <td>0.577498</td>\n",
       "      <td>0.508461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>0.578292</td>\n",
       "      <td>0.797241</td>\n",
       "      <td>0.797241</td>\n",
       "      <td>0.642547</td>\n",
       "      <td>0.642547</td>\n",
       "      <td>0.653757</td>\n",
       "      <td>0.582611</td>\n",
       "      <td>0.524831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.375404</td>\n",
       "      <td>0.614985</td>\n",
       "      <td>0.797534</td>\n",
       "      <td>0.797534</td>\n",
       "      <td>0.642909</td>\n",
       "      <td>0.642909</td>\n",
       "      <td>0.654834</td>\n",
       "      <td>0.581337</td>\n",
       "      <td>0.515247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.367703</td>\n",
       "      <td>0.600765</td>\n",
       "      <td>0.797218</td>\n",
       "      <td>0.797218</td>\n",
       "      <td>0.637402</td>\n",
       "      <td>0.637402</td>\n",
       "      <td>0.649639</td>\n",
       "      <td>0.568435</td>\n",
       "      <td>0.506872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.369208</td>\n",
       "      <td>0.560511</td>\n",
       "      <td>0.798689</td>\n",
       "      <td>0.798689</td>\n",
       "      <td>0.641907</td>\n",
       "      <td>0.641907</td>\n",
       "      <td>0.653540</td>\n",
       "      <td>0.574918</td>\n",
       "      <td>0.513967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.360761</td>\n",
       "      <td>0.601343</td>\n",
       "      <td>0.797959</td>\n",
       "      <td>0.797959</td>\n",
       "      <td>0.640201</td>\n",
       "      <td>0.640201</td>\n",
       "      <td>0.652915</td>\n",
       "      <td>0.575659</td>\n",
       "      <td>0.508881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.363188</td>\n",
       "      <td>0.597933</td>\n",
       "      <td>0.795408</td>\n",
       "      <td>0.795408</td>\n",
       "      <td>0.638801</td>\n",
       "      <td>0.638801</td>\n",
       "      <td>0.651077</td>\n",
       "      <td>0.573079</td>\n",
       "      <td>0.500062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.351925</td>\n",
       "      <td>0.612017</td>\n",
       "      <td>0.795322</td>\n",
       "      <td>0.795322</td>\n",
       "      <td>0.634993</td>\n",
       "      <td>0.634993</td>\n",
       "      <td>0.647013</td>\n",
       "      <td>0.567845</td>\n",
       "      <td>0.499366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.355373</td>\n",
       "      <td>0.622581</td>\n",
       "      <td>0.790739</td>\n",
       "      <td>0.790739</td>\n",
       "      <td>0.630382</td>\n",
       "      <td>0.630382</td>\n",
       "      <td>0.642955</td>\n",
       "      <td>0.564318</td>\n",
       "      <td>0.488514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.349822</td>\n",
       "      <td>0.634445</td>\n",
       "      <td>0.792565</td>\n",
       "      <td>0.792565</td>\n",
       "      <td>0.633406</td>\n",
       "      <td>0.633406</td>\n",
       "      <td>0.645703</td>\n",
       "      <td>0.567816</td>\n",
       "      <td>0.497226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.349396</td>\n",
       "      <td>0.646258</td>\n",
       "      <td>0.790241</td>\n",
       "      <td>0.790241</td>\n",
       "      <td>0.630394</td>\n",
       "      <td>0.630394</td>\n",
       "      <td>0.642703</td>\n",
       "      <td>0.564750</td>\n",
       "      <td>0.494276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.345843</td>\n",
       "      <td>0.654948</td>\n",
       "      <td>0.792919</td>\n",
       "      <td>0.792919</td>\n",
       "      <td>0.634636</td>\n",
       "      <td>0.634636</td>\n",
       "      <td>0.647591</td>\n",
       "      <td>0.570228</td>\n",
       "      <td>0.493868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.342238</td>\n",
       "      <td>0.663346</td>\n",
       "      <td>0.792593</td>\n",
       "      <td>0.792593</td>\n",
       "      <td>0.633631</td>\n",
       "      <td>0.633631</td>\n",
       "      <td>0.646367</td>\n",
       "      <td>0.568828</td>\n",
       "      <td>0.494938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.343383</td>\n",
       "      <td>0.662157</td>\n",
       "      <td>0.792309</td>\n",
       "      <td>0.792309</td>\n",
       "      <td>0.633494</td>\n",
       "      <td>0.633494</td>\n",
       "      <td>0.646353</td>\n",
       "      <td>0.568973</td>\n",
       "      <td>0.493922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with value: 0.46859111111111107.\n",
      "Training done\n"
     ]
    }
   ],
   "source": [
    "# Train - val split\n",
    "fold =0\n",
    "trn_ds = DataV0(trn_df, True)\n",
    "vld_ds = DataV0(val_df)\n",
    "\n",
    "trn_dl = DataLoader(\n",
    "    trn_ds,\n",
    "    batch_size=CFG.bs,\n",
    "    shuffle=True,\n",
    "    num_workers=CFG.nw,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "vld_dl = DataLoader(\n",
    "    vld_ds,\n",
    "    batch_size=CFG.bs,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.nw,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "custom_model = ViTransformerWrapper(\n",
    "    image_size = 360,\n",
    "    patch_size = 10,\n",
    "    channels = 2,\n",
    "    attn_layers = Encoder(\n",
    "        dim = 1024,\n",
    "        depth = 8,\n",
    "        heads = 16\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "opt = torch.optim.AdamW(custom_model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\n",
    "loss_func = BCEWithLogitsLossFlat()\n",
    "warmup_steps = int(len(trn_dl) * int(CFG.warmup_pct * CFG.epoch))\n",
    "total_steps = int(len(trn_dl) * CFG.epoch)\n",
    "sched = get_linear_schedule_with_warmup(\n",
    "    opt, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "fit_mixup(\n",
    "    epochs=CFG.epoch,\n",
    "    model=custom_model,\n",
    "    train_dl=trn_dl,\n",
    "    valid_dl=vld_dl,\n",
    "    loss_fn=loss_func,\n",
    "    opt=opt,\n",
    "    val_df=val_df,\n",
    "    metric=custom_auc_score,\n",
    "    folder=CFG.folder,\n",
    "    exp_name=f\"{CFG.exp_name}_{fold}\",\n",
    "    device=\"cuda:0\",\n",
    "    sched=sched,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d71ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b604f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
