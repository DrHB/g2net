{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd95f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "import torch.nn as nn\n",
    "from thop import profile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from einops import repeat\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers.optimization import (\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import (\n",
    "    LabelSmoothingCrossEntropy,\n",
    "    BinaryCrossEntropy,\n",
    "    SoftTargetCrossEntropy,\n",
    ")\n",
    "from einops import rearrange\n",
    "from fastai.vision.all import BCEWithLogitsLossFlat\n",
    "\n",
    "#https://github.com/FanChiMao/SRMNet/blob/main/SRMNet.py\n",
    "##---------- Basic Layers ----------\n",
    "def conv3x3(in_chn, out_chn, bias=True):\n",
    "    layer = nn.Conv2d(in_chn, out_chn, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size, bias=False, stride=1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        padding=(kernel_size // 2),\n",
    "        bias=bias,\n",
    "        stride=stride,\n",
    "    )\n",
    "\n",
    "\n",
    "def bili_resize(factor):\n",
    "    return nn.Upsample(scale_factor=factor, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "\n",
    "##---------- Basic Blocks ----------\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, downsample):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.block = SK_RDB(in_channels=in_size, growth_rate=out_size, num_layers=3)\n",
    "        if downsample:\n",
    "            self.downsample = PS_down(out_size, out_size, downscale=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        if self.downsample:\n",
    "            out_down = self.downsample(out)\n",
    "            return out_down, out\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        # self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2, bias=True)\n",
    "        self.up = PS_up(in_size, out_size, upscale=2)\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, False)\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        out = torch.cat([up, bridge], dim=1)\n",
    "        out = self.conv_block(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "##---------- Resizing Modules (Pixel(Un)Shuffle) ----------\n",
    "class PS_down(nn.Module):\n",
    "    def __init__(self, in_size, out_size, downscale):\n",
    "        super(PS_down, self).__init__()\n",
    "        self.UnPS = nn.PixelUnshuffle(downscale)\n",
    "        self.conv1 = nn.Conv2d((downscale**2) * in_size, out_size, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.UnPS(x)  # h/2, w/2, 4*c\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PS_up(nn.Module):\n",
    "    def __init__(self, in_size, out_size, upscale):\n",
    "        super(PS_up, self).__init__()\n",
    "\n",
    "        self.PS = nn.PixelShuffle(upscale)\n",
    "        self.conv1 = nn.Conv2d(in_size // (upscale**2), out_size, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.PS(x)  # h/2, w/2, 4*c\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "##---------- Selective Kernel Feature Fusion (SKFF) ----------\n",
    "class SKFF(nn.Module):\n",
    "    def __init__(self, in_channels, height=3, reduction=8, bias=False):\n",
    "        super(SKFF, self).__init__()\n",
    "\n",
    "        self.height = height\n",
    "        d = max(int(in_channels / reduction), 4)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_du = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, d, 1, padding=0, bias=bias), nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.fcs = nn.ModuleList([])\n",
    "        for i in range(self.height):\n",
    "            self.fcs.append(\n",
    "                nn.Conv2d(d, in_channels, kernel_size=1, stride=1, bias=bias)\n",
    "            )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, inp_feats):\n",
    "        batch_size, n_feats, H, W = inp_feats[1].shape\n",
    "\n",
    "        inp_feats = torch.cat(inp_feats, dim=1)\n",
    "        inp_feats = inp_feats.view(\n",
    "            batch_size, self.height, n_feats, inp_feats.shape[2], inp_feats.shape[3]\n",
    "        )\n",
    "\n",
    "        feats_U = torch.sum(inp_feats, dim=1)\n",
    "        feats_S = self.avg_pool(feats_U)\n",
    "        feats_Z = self.conv_du(feats_S)\n",
    "\n",
    "        attention_vectors = [fc(feats_Z) for fc in self.fcs]\n",
    "        attention_vectors = torch.cat(attention_vectors, dim=1)\n",
    "        attention_vectors = attention_vectors.view(\n",
    "            batch_size, self.height, n_feats, 1, 1\n",
    "        )\n",
    "\n",
    "        attention_vectors = self.softmax(attention_vectors)\n",
    "        feats_V = torch.sum(inp_feats * attention_vectors, dim=1)\n",
    "\n",
    "        return feats_V\n",
    "\n",
    "\n",
    "##---------- Dense Block ----------\n",
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, I):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=3 // 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sk = SKFF(out_channels, height=2, reduction=8, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.relu(self.conv(x))\n",
    "        # output = torch.cat([x, x1], 1) # -> RDB\n",
    "        output = self.sk((x, x1))\n",
    "        return output\n",
    "\n",
    "\n",
    "##---------- Selective Kernel Residual Dense Block (SK-RDB) ----------\n",
    "class SK_RDB(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_layers):\n",
    "        super(SK_RDB, self).__init__()\n",
    "        self.identity = nn.Conv2d(in_channels, growth_rate, 1, 1, 0)\n",
    "        self.layers = nn.Sequential(\n",
    "            *[DenseLayer(in_channels, in_channels, I=i) for i in range(num_layers)]\n",
    "        )\n",
    "        self.lff = nn.Conv2d(in_channels, growth_rate, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.identity(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.lff(x)\n",
    "        return res + x\n",
    "\n",
    "\n",
    "##---------- testNet ----------\n",
    "class SRMNet(nn.Module):\n",
    "    def __init__(self, in_chn=3, wf=96, depth=4):\n",
    "        super(SRMNet, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.down_path = nn.ModuleList()\n",
    "        self.bili_down = bili_resize(0.5)\n",
    "        self.conv_01 = nn.Conv2d(in_chn, wf, 3, 1, 1)\n",
    "\n",
    "        # encoder of UNet\n",
    "        prev_channels = 0\n",
    "        for i in range(depth):  # 0,1,2,3\n",
    "            downsample = True if (i + 1) < depth else False\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels + wf, (2**i) * wf, downsample)\n",
    "            )\n",
    "            prev_channels = (2**i) * wf\n",
    "\n",
    "        # decoder of UNet\n",
    "        self.up_path = nn.ModuleList()\n",
    "        self.skip_conv = nn.ModuleList()\n",
    "        self.conv_up = nn.ModuleList()\n",
    "        self.bottom_conv = nn.Conv2d(prev_channels, wf, 3, 1, 1)\n",
    "        self.bottom_up = bili_resize(2 ** (depth - 1))\n",
    "\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(UNetUpBlock(prev_channels, (2**i) * wf))\n",
    "            self.skip_conv.append(nn.Conv2d((2**i) * wf, (2**i) * wf, 3, 1, 1))\n",
    "            self.conv_up.append(\n",
    "                nn.Sequential(\n",
    "                    *[nn.Conv2d((2**i) * wf, wf, 3, 1, 1), bili_resize(2**i)]\n",
    "                )\n",
    "            )\n",
    "            prev_channels = (2**i) * wf\n",
    "\n",
    "        self.final_ff = SKFF(in_channels=wf, height=depth)\n",
    "        self.last = conv3x3(prev_channels, in_chn, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        img = x\n",
    "        scale_img = img\n",
    "\n",
    "        ##### shallow conv #####\n",
    "        x1 = self.conv_01(img)\n",
    "        encs = []\n",
    "        ######## UNet ########\n",
    "        # Down-path (Encoder)\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            if i == 0:\n",
    "                x1, x1_up = down(x1)\n",
    "                encs.append(x1_up)\n",
    "            elif (i + 1) < self.depth:\n",
    "                scale_img = self.bili_down(scale_img)\n",
    "                left_bar = self.conv_01(scale_img)\n",
    "                x1 = torch.cat([x1, left_bar], dim=1)\n",
    "                x1, x1_up = down(x1)\n",
    "                encs.append(x1_up)\n",
    "            else:\n",
    "                scale_img = self.bili_down(scale_img)\n",
    "                left_bar = self.conv_01(scale_img)\n",
    "                x1 = torch.cat([x1, left_bar], dim=1)\n",
    "                x1 = down(x1)\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "421402e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALIZATIOn\n",
    "def normalize(x, pmin=3, pmax=97, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n",
    "    \"\"\"Percentile-based image normalization.\"\"\"\n",
    "\n",
    "    mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n",
    "    ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n",
    "    return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n",
    "\n",
    "\n",
    "def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):\n",
    "    if dtype is not None:\n",
    "        x   = x.astype(dtype,copy=False)\n",
    "        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n",
    "        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n",
    "        eps = dtype(eps)\n",
    "\n",
    "    try:\n",
    "        import numexpr\n",
    "        x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n",
    "    except ImportError:\n",
    "        x =                   (x - mi) / ( ma - mi + eps )\n",
    "\n",
    "    if clip:\n",
    "        x = np.clip(x,0,1)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be61764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PIPELINE\n",
    "\n",
    "def get_snr(left, right, df):\n",
    "    df_ = pd.concat([df.query(f\"snr>{left} & snr<{right}\"), df.query(\"snr==0\")])\n",
    "    return df_\n",
    "\n",
    "\n",
    "def generate_report(df, p, fn):\n",
    "    pred = torch.sigmoid(p).cpu().numpy().reshape(-1)\n",
    "    val_df_eval = df.copy()\n",
    "    val_df_eval[\"pred\"] = pred\n",
    "    val_df_eval.to_csv(f\"{fn}_oof.csv\")\n",
    "\n",
    "    roc_100 = roc_auc_score(val_df_eval[\"target\"], val_df_eval[\"pred\"])\n",
    "    roc_50_100 = roc_auc_score(\n",
    "        get_snr(0, 100, val_df_eval)[\"target\"], get_snr(0, 100, val_df_eval)[\"pred\"]\n",
    "    )\n",
    "    roc_0_50 = roc_auc_score(\n",
    "        get_snr(0, 50, val_df_eval)[\"target\"], get_snr(0, 50, val_df_eval)[\"pred\"]\n",
    "    )\n",
    "    roc_0_40 = roc_auc_score(\n",
    "        get_snr(0, 40, val_df_eval)[\"target\"], get_snr(0, 40, val_df_eval)[\"pred\"]\n",
    "    )\n",
    "    roc_0_30 = roc_auc_score(\n",
    "        get_snr(0, 30, val_df_eval)[\"target\"], get_snr(0, 30, val_df_eval)[\"pred\"]\n",
    "    )\n",
    "\n",
    "    roc_0_20 = roc_auc_score(\n",
    "        get_snr(0, 20, val_df_eval)[\"target\"], get_snr(0, 20, val_df_eval)[\"pred\"]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"roc_all\": roc_100,\n",
    "        \"roc_50_100\": roc_50_100,\n",
    "        \"roc_0_50\": roc_0_50,\n",
    "        \"roc_0_40\": roc_0_40,\n",
    "        \"roc_0_30\": roc_0_30,\n",
    "        \"roc_0_20\": roc_0_20,\n",
    "    }\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"roc_all\": roc_100,\n",
    "        \"roc_50_100\": roc_50_100,\n",
    "        \"roc_0_50\": roc_0_50,\n",
    "        \"roc_0_40\": roc_0_40,\n",
    "        \"roc_0_30\": roc_0_30,\n",
    "        \"roc_0_20\": roc_0_20,\n",
    "    }\n",
    "\n",
    "\n",
    "class SaveModel:\n",
    "    def __init__(self, folder, exp_name, best=np.inf):\n",
    "        self.best = best\n",
    "        self.folder = Path(folder) / f\"{exp_name}.pth\"\n",
    "\n",
    "    def __call__(self, score, model, epoch):\n",
    "        if score < self.best:\n",
    "            self.best = score\n",
    "            print(f\"Better model found at epoch {epoch} with value: {self.best}.\")\n",
    "            torch.save(model.state_dict(), self.folder)\n",
    "\n",
    "\n",
    "class SaveModelMetric:\n",
    "    def __init__(self, folder, exp_name, best=-np.inf):\n",
    "        self.best = best\n",
    "        self.folder = Path(folder) / f\"{exp_name}.pth\"\n",
    "\n",
    "    def __call__(self, score, model, epoch):\n",
    "        if score > self.best:\n",
    "            self.best = score\n",
    "            print(f\"Better model found at epoch {epoch} with value: {self.best}.\")\n",
    "            torch.save(model.state_dict(), self.folder)\n",
    "\n",
    "\n",
    "class SaveModelEpoch:\n",
    "    def __init__(self, folder, exp_name, best=-np.inf):\n",
    "        self.best = best\n",
    "        self.folder = Path(folder)\n",
    "        self.exp_name = exp_name\n",
    "\n",
    "    def __call__(self, score, model, epoch):\n",
    "        self.best = score\n",
    "        print(f\"Better model found at epoch {epoch} with value: {self.best}.\")\n",
    "        torch.save(model.state_dict(), f\"{self.folder/self.exp_name}_{epoch}.pth\")\n",
    "\n",
    "\n",
    "def custom_auc_score(p, gt):\n",
    "    return roc_auc_score(gt.cpu().numpy(),  torch.sigmoid(p).cpu().numpy().reshape(-1))\n",
    "\n",
    "\n",
    "def fit_mixup(\n",
    "    epochs,\n",
    "    model,\n",
    "    train_dl,\n",
    "    valid_dl,\n",
    "    loss_fn,\n",
    "    opt,\n",
    "    metric,\n",
    "    val_df,\n",
    "    folder=\"models\",\n",
    "    exp_name=\"exp_00\",\n",
    "    device=None,\n",
    "    sched=None,\n",
    "    mixup_=False,\n",
    "    save_md=SaveModelEpoch,\n",
    "):\n",
    "    if device is None:\n",
    "        device = (\n",
    "            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        )\n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    loss_fn_trn = loss_fn\n",
    "    if mixup_:\n",
    "        mixup = Mixup(num_classes=2, mixup_alpha=0.4, prob=0.8)\n",
    "        loss_fn_trn = BinaryCrossEntropy()\n",
    "    mb = master_bar(range(epochs))\n",
    "\n",
    "    mb.write(\n",
    "        [\n",
    "            \"epoch\",\n",
    "            \"train_loss\",\n",
    "            \"valid_loss\",\n",
    "            \"val_metric\",\n",
    "            \"roc_all\",\n",
    "            \"roc_50_100\",\n",
    "            \"roc_0_50\",\n",
    "            \"roc_0_40\",\n",
    "            \"roc_0_30\",\n",
    "            \"roc_0_20\",\n",
    "        ],\n",
    "        table=True,\n",
    "    )\n",
    "    model.to(device)  # we have to put our model on gpu\n",
    "    #scaler = torch.cuda.amp.GradScaler()  # this for half precision training\n",
    "    save_md = save_md(folder, exp_name)\n",
    "\n",
    "    for i in mb:  # iterating  epoch\n",
    "        trn_loss, val_loss = 0.0, 0.0\n",
    "        trn_n, val_n = len(train_dl.dataset), len(valid_dl.dataset)\n",
    "        model.train()  # set model for training\n",
    "        for (xb, yb) in progress_bar(train_dl, parent=mb):\n",
    "            xb, yb = xb.to(device), yb.to(device)  # putting batches to device\n",
    "            if mixup_:\n",
    "                xb, yb = mixup(xb, yb)\n",
    "           \n",
    "            out = model(xb)  # forward pass\n",
    "            loss = loss_fn_trn(out, yb)  # calulation loss\n",
    "\n",
    "            trn_loss += loss.item()\n",
    "            #print(loss.item())\n",
    "            opt.zero_grad()  # zeroing optimizer\n",
    "            loss.backward()  # backward\n",
    "            opt.step()  # optimzers step\n",
    "            if sched is not None:\n",
    "                sched.step()  # scuedular step\n",
    "\n",
    "        trn_loss /= mb.child.total\n",
    "\n",
    "        # putting model in eval mode\n",
    "        model.eval()\n",
    "        gt = []\n",
    "        pred = []\n",
    "        # after epooch is done we can run a validation dataloder and see how are doing\n",
    "        with torch.no_grad():\n",
    "            for (xb, yb) in progress_bar(valid_dl, parent=mb):\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                loss = loss_fn(out, yb)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                gt.append(yb.detach())\n",
    "                pred.append(out.detach())\n",
    "        # calculating metric\n",
    "        metric_ = metric(torch.cat(pred), torch.cat(gt))\n",
    "        # saving model if necessary\n",
    "        save_md(metric_, model, i)\n",
    "        val_loss /= mb.child.total\n",
    "        dict_res = generate_report(val_df, torch.cat(pred), f\"{folder}/{exp_name}_{i}\")\n",
    "\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"trn_loss\": [trn_loss],\n",
    "                \"val_loss\": [val_loss],\n",
    "                \"metric\": [metric_],\n",
    "                \"roc_all\": [dict_res[\"roc_all\"]],\n",
    "                \"roc_50_100\": [dict_res[\"roc_50_100\"]],\n",
    "                \"roc_0_50\": [dict_res[\"roc_0_50\"]],\n",
    "                \"roc_0_40\": [dict_res[\"roc_0_40\"]],\n",
    "                \"roc_0_30\": [dict_res[\"roc_0_30\"]],\n",
    "                \"roc_0_20\": [dict_res[\"roc_0_20\"]],\n",
    "            }\n",
    "        ).to_csv(f\"{folder}/{exp_name}_{i}.csv\", index=False)\n",
    "        mb.write(\n",
    "            [\n",
    "                i,\n",
    "                f\"{trn_loss:.6f}\",\n",
    "                f\"{val_loss:.6f}\",\n",
    "                f\"{metric_:.6f}\",\n",
    "                f\"{dict_res['roc_all']:.6f}\",\n",
    "                f\"{dict_res['roc_50_100']:.6f}\",\n",
    "                f\"{dict_res['roc_0_50']:.6f}\",\n",
    "                f\"{dict_res['roc_0_40']:.6f}\",\n",
    "                f\"{dict_res['roc_0_30']:.6f}\",\n",
    "                f\"{dict_res['roc_0_20']:.6f}\",\n",
    "            ],\n",
    "            table=True,\n",
    "        )\n",
    "    print(\"Training done\")\n",
    "    # loading the best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb6371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VOID TO NOISE PAPER SET UP\n",
    "import copy\n",
    "import h5py\n",
    "\n",
    "\n",
    "\n",
    "class DataV0:\n",
    "    \"\"\"\n",
    "    dataset = Dataset(data_type, df)\n",
    "\n",
    "    img, y = dataset[i]\n",
    "      img (np.float32): 2 x 360 x 128\n",
    "      y (np.float32): label 0 or 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, tfms=False):\n",
    "        self.df = df\n",
    "        self.tfms = tfms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        i (int): get ith data\n",
    "        \"\"\"\n",
    "        r = self.df.iloc[i]\n",
    "        y = np.float32(r.target)\n",
    "        filename=r.id\n",
    "        file_id = Path(r.id).stem\n",
    "        img = np.empty((2, 360, 128), dtype=np.float32)\n",
    "\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            g = f[file_id]\n",
    "\n",
    "            for ch, s in enumerate(['H1', 'L1']):\n",
    "                a = g[s]['SFTs'][:, :4096] * 1e22  # Fourier coefficient complex64\n",
    "                p = a.real**2 + a.imag**2  # power\n",
    "                p = np.mean(p.reshape(360, 128, 32), axis=2)  # compress 4096 -> 128\n",
    "                img[ch] = normalize(p, clip=True)\n",
    "\n",
    "        if self.tfms:\n",
    "            if np.random.rand() <= 0.5:  # horizontal flip\n",
    "                img = np.flip(img, axis=1).copy()\n",
    "            if np.random.rand() <= 0.5:  # vertical flip\n",
    "                img = np.flip(img, axis=2).copy()\n",
    "            if np.random.rand() <= 0.5:  # vertical shift\n",
    "                img = np.roll(img, np.random.randint(low=0, high=img.shape[1]), axis=1)\n",
    "        \n",
    "        return img, y\n",
    "\n",
    "\n",
    "def customplot(x, y, k=None):\n",
    "    if k:\n",
    "        plt.imshow(torch.concat([x.mean(0), y.mean(0), k.mean(0)], 1))\n",
    "    else:\n",
    "        plt.imshow(torch.concat([x.mean(0), y.mean(0)], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82291f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    bs = 32\n",
    "    nw = 8\n",
    "    model_name = \"NOISE_TO_VOID_FROZEN\"\n",
    "    lr = 1e-4\n",
    "    wd = 1e-4\n",
    "    epoch = 30\n",
    "    warmup_pct = 0.01\n",
    "    num_classes = 2\n",
    "    dropout_rate = 0.3\n",
    "    folder = \"NOISE_TO_VOID_FROZEN\"\n",
    "    exp_name = f\"{folder}_{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97b1b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit = pd.read_csv('../data/sample_submission.csv')\n",
    "#submit.columns = ['fn', 'target']\n",
    "#submit['fn'] = submit['fn'].apply(lambda x: Path('../data/test')/f'{x}.hdf5')\n",
    "#submit.columns = ['id', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d974c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_df = pd.read_csv('../data/SPLITS/V_20/trn_df.csv')\n",
    "vld_df = pd.read_csv('../data/SPLITS/V_20/val_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9628b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>alpha</th>\n",
       "      <th>daelta</th>\n",
       "      <th>cosi</th>\n",
       "      <th>psi</th>\n",
       "      <th>phi</th>\n",
       "      <th>h0</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>snr</th>\n",
       "      <th>id_csv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/custom_data/DATA_V32/data/hb_1f75fce18f6494897815ed85156bc358_noise.h5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/custom_data/DATA_V31/data/hb_b442b78713221f1883765ba43ee5441d_noise.h5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/custom_data/DATA_V31/data/hb_363f1827421678c14c1295d692cb80a3_noise.h5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/custom_data/DATA_V32/data/hb_6f0538fcb3fd83a751c56a01c6333cee_noise.h5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/custom_data/DATA_V31/data/hb_cf8bdff17cf33ca16937a5930a08fe03.h5</td>\n",
       "      <td>1</td>\n",
       "      <td>hb_cf8bdff17cf33ca16937a5930a08fe03</td>\n",
       "      <td>5.14123</td>\n",
       "      <td>1.164792</td>\n",
       "      <td>-0.340831</td>\n",
       "      <td>0.110483</td>\n",
       "      <td>0.167176</td>\n",
       "      <td>2.443617e-25</td>\n",
       "      <td>399.367351</td>\n",
       "      <td>3.161778e-09</td>\n",
       "      <td>60.097779</td>\n",
       "      <td>../data/custom_data/DATA_V31/data/hb_cf8bdff17cf33ca16937a5930a08fe03.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               id  \\\n",
       "0  ../data/custom_data/DATA_V32/data/hb_1f75fce18f6494897815ed85156bc358_noise.h5   \n",
       "1  ../data/custom_data/DATA_V31/data/hb_b442b78713221f1883765ba43ee5441d_noise.h5   \n",
       "2  ../data/custom_data/DATA_V31/data/hb_363f1827421678c14c1295d692cb80a3_noise.h5   \n",
       "3  ../data/custom_data/DATA_V32/data/hb_6f0538fcb3fd83a751c56a01c6333cee_noise.h5   \n",
       "4        ../data/custom_data/DATA_V31/data/hb_cf8bdff17cf33ca16937a5930a08fe03.h5   \n",
       "\n",
       "   target                              uniq_id    alpha    daelta      cosi  \\\n",
       "0       0                                    0  0.00000  0.000000  0.000000   \n",
       "1       0                                    0  0.00000  0.000000  0.000000   \n",
       "2       0                                    0  0.00000  0.000000  0.000000   \n",
       "3       0                                    0  0.00000  0.000000  0.000000   \n",
       "4       1  hb_cf8bdff17cf33ca16937a5930a08fe03  5.14123  1.164792 -0.340831   \n",
       "\n",
       "        psi       phi            h0          f0            f1        snr  \\\n",
       "0  0.000000  0.000000  0.000000e+00    0.000000  0.000000e+00   0.000000   \n",
       "1  0.000000  0.000000  0.000000e+00    0.000000  0.000000e+00   0.000000   \n",
       "2  0.000000  0.000000  0.000000e+00    0.000000  0.000000e+00   0.000000   \n",
       "3  0.000000  0.000000  0.000000e+00    0.000000  0.000000e+00   0.000000   \n",
       "4  0.110483  0.167176  2.443617e-25  399.367351  3.161778e-09  60.097779   \n",
       "\n",
       "                                                                      id_csv  \n",
       "0                                                                          0  \n",
       "1                                                                          0  \n",
       "2                                                                          0  \n",
       "3                                                                          0  \n",
       "4  ../data/custom_data/DATA_V31/data/hb_cf8bdff17cf33ca16937a5930a08fe03.csv  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_df.columns\n",
    "trn_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "696e5c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveConcatPooling(nn.Module):\n",
    "    def forward(self, x):\n",
    "        avg_pool = F.adaptive_avg_pool2d(x, 1)\n",
    "        max_pool = F.adaptive_max_pool2d(x, 1)\n",
    "        return torch.cat([avg_pool, max_pool], dim=1)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        return x.view(bs, -1)\n",
    "\n",
    "\n",
    "class CustomModelSrmNet(nn.Module):\n",
    "    def __init__(self, nf=1536):\n",
    "        super().__init__()\n",
    "        self.enc = SRMNet(in_chn=2)\n",
    "        self.dec = nn.Sequential(\n",
    "            AdaptiveConcatPooling(),\n",
    "            Flatten(),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(nf, nf // 2, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.75),\n",
    "            nn.Linear(nf // 2, 1, bias=False),\n",
    "        )\n",
    "        self.enc.load_state_dict(\n",
    "            torch.load(\n",
    "                \"../EXP_40/EXP_40_01_NV_TEST_V4/EXP_40_01_NV_TEST_V4_SRG_REAL_NOISE_TO_VOID_TEST_V4_DATA_V31_9.pth\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.enc(x)\n",
    "        return self.dec(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2ebd040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>val_metric</th>\n",
       "      <th>roc_all</th>\n",
       "      <th>roc_50_100</th>\n",
       "      <th>roc_0_50</th>\n",
       "      <th>roc_0_40</th>\n",
       "      <th>roc_0_30</th>\n",
       "      <th>roc_0_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.692527</td>\n",
       "      <td>0.692117</td>\n",
       "      <td>0.623466</td>\n",
       "      <td>0.623466</td>\n",
       "      <td>0.623466</td>\n",
       "      <td>0.557336</td>\n",
       "      <td>0.556091</td>\n",
       "      <td>0.569121</td>\n",
       "      <td>0.696667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.691077</td>\n",
       "      <td>0.691236</td>\n",
       "      <td>0.628017</td>\n",
       "      <td>0.628017</td>\n",
       "      <td>0.628017</td>\n",
       "      <td>0.559442</td>\n",
       "      <td>0.557061</td>\n",
       "      <td>0.565782</td>\n",
       "      <td>0.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.688245</td>\n",
       "      <td>0.692172</td>\n",
       "      <td>0.585718</td>\n",
       "      <td>0.585718</td>\n",
       "      <td>0.585718</td>\n",
       "      <td>0.463134</td>\n",
       "      <td>0.451346</td>\n",
       "      <td>0.446280</td>\n",
       "      <td>0.412667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.686411</td>\n",
       "      <td>0.689344</td>\n",
       "      <td>0.657145</td>\n",
       "      <td>0.657145</td>\n",
       "      <td>0.657145</td>\n",
       "      <td>0.550803</td>\n",
       "      <td>0.545567</td>\n",
       "      <td>0.555185</td>\n",
       "      <td>0.705333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.682820</td>\n",
       "      <td>0.689126</td>\n",
       "      <td>0.659888</td>\n",
       "      <td>0.659888</td>\n",
       "      <td>0.659888</td>\n",
       "      <td>0.539244</td>\n",
       "      <td>0.529970</td>\n",
       "      <td>0.537027</td>\n",
       "      <td>0.712667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.679625</td>\n",
       "      <td>0.686652</td>\n",
       "      <td>0.658898</td>\n",
       "      <td>0.658898</td>\n",
       "      <td>0.658898</td>\n",
       "      <td>0.538020</td>\n",
       "      <td>0.528694</td>\n",
       "      <td>0.536383</td>\n",
       "      <td>0.730667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.674488</td>\n",
       "      <td>0.686748</td>\n",
       "      <td>0.661328</td>\n",
       "      <td>0.661328</td>\n",
       "      <td>0.661328</td>\n",
       "      <td>0.547960</td>\n",
       "      <td>0.541254</td>\n",
       "      <td>0.549864</td>\n",
       "      <td>0.710667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.671639</td>\n",
       "      <td>0.683493</td>\n",
       "      <td>0.657371</td>\n",
       "      <td>0.657371</td>\n",
       "      <td>0.657371</td>\n",
       "      <td>0.529994</td>\n",
       "      <td>0.519688</td>\n",
       "      <td>0.527267</td>\n",
       "      <td>0.746667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.667472</td>\n",
       "      <td>0.682310</td>\n",
       "      <td>0.648444</td>\n",
       "      <td>0.648444</td>\n",
       "      <td>0.648444</td>\n",
       "      <td>0.513269</td>\n",
       "      <td>0.501967</td>\n",
       "      <td>0.508340</td>\n",
       "      <td>0.757667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.665163</td>\n",
       "      <td>0.681024</td>\n",
       "      <td>0.651883</td>\n",
       "      <td>0.651883</td>\n",
       "      <td>0.651883</td>\n",
       "      <td>0.517548</td>\n",
       "      <td>0.506708</td>\n",
       "      <td>0.512860</td>\n",
       "      <td>0.755000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.660799</td>\n",
       "      <td>0.678650</td>\n",
       "      <td>0.660275</td>\n",
       "      <td>0.660275</td>\n",
       "      <td>0.660275</td>\n",
       "      <td>0.532910</td>\n",
       "      <td>0.523252</td>\n",
       "      <td>0.530531</td>\n",
       "      <td>0.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.657081</td>\n",
       "      <td>0.677723</td>\n",
       "      <td>0.655451</td>\n",
       "      <td>0.655451</td>\n",
       "      <td>0.655451</td>\n",
       "      <td>0.521982</td>\n",
       "      <td>0.511368</td>\n",
       "      <td>0.517502</td>\n",
       "      <td>0.758333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.653027</td>\n",
       "      <td>0.677571</td>\n",
       "      <td>0.664200</td>\n",
       "      <td>0.664200</td>\n",
       "      <td>0.664200</td>\n",
       "      <td>0.543527</td>\n",
       "      <td>0.535884</td>\n",
       "      <td>0.543004</td>\n",
       "      <td>0.719667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.651938</td>\n",
       "      <td>0.677140</td>\n",
       "      <td>0.654279</td>\n",
       "      <td>0.654279</td>\n",
       "      <td>0.654279</td>\n",
       "      <td>0.517251</td>\n",
       "      <td>0.506044</td>\n",
       "      <td>0.511370</td>\n",
       "      <td>0.743000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.649904</td>\n",
       "      <td>0.673376</td>\n",
       "      <td>0.664381</td>\n",
       "      <td>0.664381</td>\n",
       "      <td>0.664381</td>\n",
       "      <td>0.539873</td>\n",
       "      <td>0.531444</td>\n",
       "      <td>0.538551</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.646418</td>\n",
       "      <td>0.673287</td>\n",
       "      <td>0.664486</td>\n",
       "      <td>0.664486</td>\n",
       "      <td>0.664486</td>\n",
       "      <td>0.536677</td>\n",
       "      <td>0.527362</td>\n",
       "      <td>0.534412</td>\n",
       "      <td>0.727333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.644910</td>\n",
       "      <td>0.671804</td>\n",
       "      <td>0.657456</td>\n",
       "      <td>0.657456</td>\n",
       "      <td>0.657456</td>\n",
       "      <td>0.520355</td>\n",
       "      <td>0.508999</td>\n",
       "      <td>0.514716</td>\n",
       "      <td>0.743333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.643786</td>\n",
       "      <td>0.671917</td>\n",
       "      <td>0.656182</td>\n",
       "      <td>0.656182</td>\n",
       "      <td>0.656182</td>\n",
       "      <td>0.516889</td>\n",
       "      <td>0.505241</td>\n",
       "      <td>0.510593</td>\n",
       "      <td>0.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.642481</td>\n",
       "      <td>0.670507</td>\n",
       "      <td>0.658653</td>\n",
       "      <td>0.658653</td>\n",
       "      <td>0.658653</td>\n",
       "      <td>0.520040</td>\n",
       "      <td>0.508679</td>\n",
       "      <td>0.514481</td>\n",
       "      <td>0.738000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.639349</td>\n",
       "      <td>0.672077</td>\n",
       "      <td>0.664743</td>\n",
       "      <td>0.664743</td>\n",
       "      <td>0.664743</td>\n",
       "      <td>0.530790</td>\n",
       "      <td>0.520336</td>\n",
       "      <td>0.526963</td>\n",
       "      <td>0.732667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.639732</td>\n",
       "      <td>0.669577</td>\n",
       "      <td>0.663486</td>\n",
       "      <td>0.663486</td>\n",
       "      <td>0.663486</td>\n",
       "      <td>0.527842</td>\n",
       "      <td>0.516946</td>\n",
       "      <td>0.523292</td>\n",
       "      <td>0.736333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.639150</td>\n",
       "      <td>0.669856</td>\n",
       "      <td>0.661766</td>\n",
       "      <td>0.661766</td>\n",
       "      <td>0.661766</td>\n",
       "      <td>0.523576</td>\n",
       "      <td>0.512293</td>\n",
       "      <td>0.518230</td>\n",
       "      <td>0.734333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.637949</td>\n",
       "      <td>0.667124</td>\n",
       "      <td>0.665325</td>\n",
       "      <td>0.665325</td>\n",
       "      <td>0.665325</td>\n",
       "      <td>0.530725</td>\n",
       "      <td>0.520008</td>\n",
       "      <td>0.526877</td>\n",
       "      <td>0.731667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.636599</td>\n",
       "      <td>0.670359</td>\n",
       "      <td>0.665050</td>\n",
       "      <td>0.665050</td>\n",
       "      <td>0.665050</td>\n",
       "      <td>0.527956</td>\n",
       "      <td>0.517007</td>\n",
       "      <td>0.522922</td>\n",
       "      <td>0.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.635885</td>\n",
       "      <td>0.667915</td>\n",
       "      <td>0.660961</td>\n",
       "      <td>0.660961</td>\n",
       "      <td>0.660961</td>\n",
       "      <td>0.521604</td>\n",
       "      <td>0.510174</td>\n",
       "      <td>0.516117</td>\n",
       "      <td>0.731000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.635766</td>\n",
       "      <td>0.666680</td>\n",
       "      <td>0.662686</td>\n",
       "      <td>0.662686</td>\n",
       "      <td>0.662686</td>\n",
       "      <td>0.524505</td>\n",
       "      <td>0.513226</td>\n",
       "      <td>0.519428</td>\n",
       "      <td>0.735000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.634049</td>\n",
       "      <td>0.667326</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.664575</td>\n",
       "      <td>0.527816</td>\n",
       "      <td>0.516764</td>\n",
       "      <td>0.523235</td>\n",
       "      <td>0.735333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.635678</td>\n",
       "      <td>0.666754</td>\n",
       "      <td>0.662426</td>\n",
       "      <td>0.662426</td>\n",
       "      <td>0.662426</td>\n",
       "      <td>0.523567</td>\n",
       "      <td>0.512289</td>\n",
       "      <td>0.518300</td>\n",
       "      <td>0.732667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.633800</td>\n",
       "      <td>0.667340</td>\n",
       "      <td>0.662910</td>\n",
       "      <td>0.662910</td>\n",
       "      <td>0.662910</td>\n",
       "      <td>0.524171</td>\n",
       "      <td>0.512893</td>\n",
       "      <td>0.518926</td>\n",
       "      <td>0.732667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.633964</td>\n",
       "      <td>0.667312</td>\n",
       "      <td>0.662324</td>\n",
       "      <td>0.662324</td>\n",
       "      <td>0.662324</td>\n",
       "      <td>0.523104</td>\n",
       "      <td>0.511788</td>\n",
       "      <td>0.517761</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with value: 0.6234662222222223.\n",
      "Better model found at epoch 1 with value: 0.6280168888888888.\n",
      "Better model found at epoch 2 with value: 0.5857177777777778.\n",
      "Better model found at epoch 3 with value: 0.6571453333333334.\n",
      "Better model found at epoch 4 with value: 0.6598877777777779.\n",
      "Better model found at epoch 5 with value: 0.658898.\n",
      "Better model found at epoch 6 with value: 0.6613282222222221.\n",
      "Better model found at epoch 7 with value: 0.6573706666666667.\n",
      "Better model found at epoch 8 with value: 0.6484437777777778.\n",
      "Better model found at epoch 9 with value: 0.6518828888888889.\n",
      "Better model found at epoch 10 with value: 0.6602753333333333.\n",
      "Better model found at epoch 11 with value: 0.6554513333333334.\n",
      "Better model found at epoch 12 with value: 0.6642.\n",
      "Better model found at epoch 13 with value: 0.654279111111111.\n",
      "Better model found at epoch 14 with value: 0.6643813333333333.\n",
      "Better model found at epoch 15 with value: 0.6644855555555556.\n",
      "Better model found at epoch 16 with value: 0.6574555555555556.\n",
      "Better model found at epoch 17 with value: 0.656182.\n",
      "Better model found at epoch 18 with value: 0.6586533333333333.\n",
      "Better model found at epoch 19 with value: 0.6647431111111111.\n",
      "Better model found at epoch 20 with value: 0.6634862222222222.\n",
      "Better model found at epoch 21 with value: 0.6617657777777778.\n",
      "Better model found at epoch 22 with value: 0.6653246666666667.\n",
      "Better model found at epoch 23 with value: 0.6650497777777777.\n",
      "Better model found at epoch 24 with value: 0.6609608888888889.\n",
      "Better model found at epoch 25 with value: 0.6626864444444445.\n",
      "Better model found at epoch 26 with value: 0.6645748888888889.\n",
      "Better model found at epoch 27 with value: 0.6624264444444444.\n",
      "Better model found at epoch 28 with value: 0.6629102222222222.\n",
      "Better model found at epoch 29 with value: 0.662324.\n",
      "Training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = []\n",
    "preds = []\n",
    "fold = 0\n",
    "\n",
    "# Train - val split\n",
    "trn_ds = DataV0(trn_df)\n",
    "vld_ds = DataV0(vld_df)\n",
    "\n",
    "trn_dl = DataLoader(\n",
    "    trn_ds,\n",
    "    batch_size=CFG.bs,\n",
    "    shuffle=True,\n",
    "    num_workers=CFG.nw,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "vld_dl = DataLoader(\n",
    "    vld_ds,\n",
    "    batch_size=CFG.bs,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.nw,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "custom_model = CustomModelSrmNet()\n",
    "opt = torch.optim.AdamW(custom_model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\n",
    "loss_func = BCEWithLogitsLossFlat()\n",
    "warmup_steps = int(len(trn_dl) * int(CFG.warmup_pct * CFG.epoch))\n",
    "total_steps = int(len(trn_dl) * CFG.epoch)\n",
    "sched = get_linear_schedule_with_warmup(\n",
    "    opt, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "fit_mixup(\n",
    "    epochs=CFG.epoch,\n",
    "    model=custom_model,\n",
    "    train_dl=trn_dl,\n",
    "    valid_dl=vld_dl,\n",
    "    loss_fn=loss_func,\n",
    "    opt=opt,\n",
    "    val_df = vld_df,\n",
    "    metric=custom_auc_score,\n",
    "    folder=CFG.folder,\n",
    "    exp_name=f\"{CFG.exp_name}_{fold}\",\n",
    "    device=\"cuda:0\",\n",
    "    sched=sched,\n",
    ")\n",
    "del custom_model\n",
    "del trn_dl\n",
    "del vld_dl\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9ae5815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No traceback has been produced, nothing to debug.\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1dfe29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
