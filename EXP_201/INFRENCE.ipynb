{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e383b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import copy\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.cuda.set_device(1)\n",
    "from pdb import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from x_transformers import  Encoder, Decoder\n",
    "from x_transformers.autoregressive_wrapper import exists\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from fastai.vision.all import BCEWithLogitsLossFlat\n",
    "from transformers.optimization import (\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "import os\n",
    "from timm import create_model\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ab37361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    bs = 32\n",
    "    nw = 4\n",
    "    model_name = \"convmixer_1536_20\"\n",
    "    lr = 1e-4\n",
    "    wd = 1e-4\n",
    "    epoch = 12\n",
    "    warmup_pct = 0.1\n",
    "    num_classes = 1\n",
    "    dropout_rate = 0.3\n",
    "    folder = \"EXP_200_BASELINE_CASHE_V3\"\n",
    "    mixup=False\n",
    "    exp_name = f\"{folder}_{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a785480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snr(left, right, df):\n",
    "    df_ = pd.concat([df.query(f\"snr>{left} & snr<{right}\"), df.query(\"snr==0\")])\n",
    "    return df_\n",
    "\n",
    "\n",
    "def generate_report(df):\n",
    "    val_df_eval = df.copy()\n",
    "\n",
    "    roc_100 = roc_auc_score(val_df_eval[\"target\"], val_df_eval[\"pred\"])\n",
    "\n",
    "    roc_25_50 = roc_auc_score(\n",
    "        get_snr(30, 50, val_df_eval)[\"target\"], get_snr(30, 50, val_df_eval)[\"pred\"]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"roc_all\": roc_100,\n",
    "        \"roc_30_50\": roc_25_50,\n",
    "\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3eed83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_mask(spec, T=10):\n",
    "    cloned = spec.clone().detach()\n",
    "    len_spectro = cloned.shape[2]\n",
    "    num_masks = np.random.randint(3, 8)\n",
    "    for i in range(0, num_masks):\n",
    "        t = random.randrange(0, T)\n",
    "        t_zero = random.randrange(0, len_spectro - t)\n",
    "\n",
    "        # avoids randrange error if values are equal and range is empty\n",
    "        if (t_zero == t_zero + t): return cloned\n",
    "\n",
    "        mask_end = random.randrange(t_zero, t_zero + t)\n",
    "        cloned[:, :,t_zero:mask_end] = 0\n",
    "    return cloned\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def freq_mask(spec, F=30):\n",
    "    cloned = spec.clone().detach()\n",
    "    num_mel_channels = cloned.shape[1]\n",
    "    num_masks = np.random.randint(3, 8)\n",
    "    for i in range(0, num_masks):        \n",
    "        f = random.randrange(0, F)\n",
    "        f_zero = random.randrange(0, num_mel_channels - f)\n",
    "\n",
    "        # avoids randrange error if values are equal and range is empty\n",
    "        if (f_zero == f_zero + f): return cloned\n",
    "\n",
    "        mask_end = random.randrange(f_zero, f_zero + f) \n",
    "        cloned[:, f_zero:mask_end, :] = 0\n",
    "    \n",
    "    return cloned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e79a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pkl(filename):   \n",
    "    with open(filename, 'rb') as file1: \n",
    "        k = pickle.load(file1)\n",
    "        h1 = k[\"H1\"]['spectrogram']\n",
    "        l1 = k[\"L1\"]['spectrogram']\n",
    "        h1_timestamp = k[\"H1\"]['timestamps']\n",
    "        l1_timestamp = k[\"L1\"]['timestamps']\n",
    "        freq = k['frequency']\n",
    "        \n",
    "    data_dict = {\"sft\" : np.stack([h1[:, :4096], l1[:, :4096]]), \n",
    "                 \"timestamps\": {\"H1\": h1_timestamp, \n",
    "                                    \"L1\": l1_timestamp}}\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d80e7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sft):\n",
    "    sft = sft * 1e22\n",
    "    sft = sft.real**2 + sft.imag**2\n",
    "    return sft\n",
    "\n",
    "\n",
    "def normalize(data):\n",
    "    data[0] = data[0] / data[0].mean()\n",
    "    data[1] = data[1] / data[1].mean()\n",
    "    data = data.reshape(2, 360, 128, 32).mean(-1)  # compress 4096 -> 128\n",
    "    data = data - data.mean()\n",
    "    data = data / data.std()\n",
    "    return torch.tensor(data)\n",
    "\n",
    "\n",
    "def read_h5(file):\n",
    "    file = Path(file)\n",
    "    with h5py.File(file, \"r\") as f:\n",
    "        filename = file.stem\n",
    "        k = f[filename]\n",
    "        h1 = k[\"H1\"]\n",
    "        l1 = k[\"L1\"]\n",
    "        h1_stft = h1[\"SFTs\"][()]\n",
    "        h1_timestamp = h1[\"timestamps_GPS\"][()]\n",
    "        l1_stft = l1[\"SFTs\"][()]\n",
    "        l1_timestamp = l1[\"timestamps_GPS\"][()]\n",
    "        \n",
    "        data_dict = {\"sft\" : np.stack([h1_stft[:, :4096], l1_stft[:, :4096]]), \n",
    "                 \"timestamps\": {\"H1\": h1_timestamp, \n",
    "                                    \"L1\": l1_timestamp}}\n",
    "        \n",
    "        return data_dict\n",
    "    \n",
    "\n",
    "    \n",
    "class ValLoader(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    dataset = Dataset(data_type, df)\n",
    "\n",
    "    img, y = dataset[i]\n",
    "      img (np.float32): 2 x 360 x 128\n",
    "      y (np.float32): label 0 or 1\n",
    "    \"\"\"\n",
    "    def __init__(self, df, freq_tfms=False):\n",
    "        self.df = df\n",
    "        self.tfms = freq_tfms\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        i (int): get ith data\n",
    "        \"\"\"\n",
    "        r = self.df.iloc[i]\n",
    "        y = np.float32(r.target)\n",
    "        img = normalize(preprocess(read_h5(r.id)['sft']))\n",
    "        return img, y\n",
    "    \n",
    "    \n",
    "class ValLoaderPickle(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    dataset = Dataset(data_type, df)\n",
    "\n",
    "    img, y = dataset[i]\n",
    "      img (np.float32): 2 x 360 x 128\n",
    "      y (np.float32): label 0 or 1\n",
    "    \"\"\"\n",
    "    def __init__(self, df, freq_tfms=False):\n",
    "        self.df = df\n",
    "        self.tfms = freq_tfms\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        i (int): get ith data\n",
    "        \"\"\"\n",
    "        r = self.df.iloc[i]\n",
    "        y = np.float32(r.target)\n",
    "        img = normalize(read_pkl(str(r.id))['sft'])\n",
    "        return img.float(), y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7faec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_rot90_cw(x):\n",
    "    return x.rot90(k=-1, dims=(2, 3))\n",
    "\n",
    "\n",
    "def torch_fliplr(x: Tensor):\n",
    "    \"\"\"\n",
    "    Flip 4D image tensor horizontally\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return x.flip(3)\n",
    "\n",
    "\n",
    "def torch_flipud(x: Tensor):\n",
    "    \"\"\"\n",
    "    Flip 4D image tensor vertically\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return x.flip(2)\n",
    "\n",
    "\n",
    "def tencrop_image2label(model: nn.Module, image: Tensor) -> Tensor:\n",
    "    \"\"\"Test-time augmentation for image classification that takes five crops out of input tensor (4 on corners and central)\n",
    "    and averages predictions from them and from their horisontally-flipped versions (10-Crop TTA).\n",
    "    :param model: Classification model\n",
    "    :param image: Input image tensor\n",
    "    :param crop_size: Crop size. Must be smaller than image size\n",
    "    :return: Averaged logits\n",
    "    \"\"\"\n",
    "\n",
    "    output = (\n",
    "        torch.sigmoid(model(image))\n",
    "        + torch.sigmoid(model(torch_flipud(image)))\n",
    "        + torch.sigmoid(model(torch_fliplr(image)))\n",
    "        + torch.sigmoid(model(torch_flipud(torch_fliplr(image))))\n",
    "    ) / 4.\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7005f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tta(dl, model):\n",
    "    res = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(vld_dl):\n",
    "            out = tencrop_image2label(model, x.cuda()).detach().cpu()\n",
    "            #out = torch.sigmoid(model(x.cuda())).detach().cpu()\n",
    "            res.append(out)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b5a5a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 147/147 [03:45<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "real_noise_fns = sorted(\n",
    "    Path(\"../data/custom_data/DATA_V33/data/\").glob(\"*.pth\"),\n",
    "    key=lambda x: str(x).split(\"_\")[-2],\n",
    ")\n",
    "\n",
    "fake_noise_fns = sorted(\n",
    "    Path(\"../data/custom_data/DATA_V34/data/\").glob(\"*.pth\"),\n",
    "    key=lambda x: str(x).split(\"_\")[-2],\n",
    ")\n",
    "\n",
    "\n",
    "noise = (\n",
    "    list(Path(\"../data/custom_data/DATA_V31_V32_NOISE\").glob(\"*.pth\"))\n",
    "    + real_noise_fns[:1100]\n",
    "    + fake_noise_fns\n",
    ")\n",
    "cashe_fns = list(Path(\"cashe_dataset\").glob(\"*.pth\"))\n",
    "\n",
    "val_df = pd.read_csv(\"../data/SPLITS/V_22/val_df.csv\")\n",
    "comp_train = pd.read_csv(\"../data/train_labels.csv\")\n",
    "comp_train.columns = [\"fn\", \"target\"]\n",
    "comp_train = comp_train.query(\"target>=0\")\n",
    "comp_train[\"fn\"] = comp_train[\"fn\"].apply(lambda x: Path(\"../data/train\") / f\"{x}.hdf5\")\n",
    "comp_train.columns = [\"id\", \"target\"]\n",
    "comp_train[\"data_type\"] = \"comp_train\"\n",
    "real_noise_df = pd.DataFrame({\"id\": real_noise_fns[1100:], \"target\": 0.0, \"snr\": 0})\n",
    "real_noise_df[\"id\"] = real_noise_df[\"id\"].apply(\n",
    "    lambda x: Path(str(x).replace(\".pth\", \".h5\"))\n",
    ")\n",
    "\n",
    "val_df = pd.concat([val_df, comp_train, real_noise_df], ignore_index=True)\n",
    "val_df['id']= val_df['id'].apply(lambda x: Path(x))\n",
    "                             \n",
    "fns = [\"EXP_200_BASELINE_CASHE_V6/EXP_200_BASELINE_CASHE_V6_convmixer_1536_20_0_13.pth\"]\n",
    "\n",
    "custom_model = create_model(\n",
    "                    CFG.model_name,\n",
    "                    pretrained=True,\n",
    "                    num_classes=1,\n",
    "                    in_chans=2,\n",
    "                )\n",
    "\n",
    "custom_model.load_state_dict(torch.load(fns[0]))\n",
    "custom_model.cuda();\n",
    "custom_model.eval();\n",
    "sub_ds = ValLoader(val_df)\n",
    "vld_dl = DataLoader(\n",
    "    sub_ds,\n",
    "    batch_size=CFG.bs,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.nw,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "res = predict_tta(vld_dl, custom_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3a459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "416929a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['pred'] = torch.cat(res).view(-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b8ac135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9077064775101727"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(val_df['target'], val_df['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c82214d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8256374999999999"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(val_df.query('data_type == \"comp_train\"')['target'], \n",
    "              val_df.query('data_type == \"comp_train\"')['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "042fbd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [06:21<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "fns = [\"EXP_200_BASELINE_CASHE_V6/EXP_200_BASELINE_CASHE_V6_convmixer_1536_20_0_13.pth\"]\n",
    "\n",
    "custom_model = create_model(\n",
    "                    CFG.model_name,\n",
    "                    pretrained=True,\n",
    "                    num_classes=1,\n",
    "                    in_chans=2,\n",
    "                )\n",
    "\n",
    "custom_model.load_state_dict(torch.load(fns[0]))\n",
    "custom_model.cuda();\n",
    "sub = pd.read_csv('../data/sample_submission.csv')\n",
    "sub['id'] = sub['id'].apply(lambda x: Path(f'../data/test/{x}.hdf5'))\n",
    "sub_ds = ValLoader(sub)\n",
    "vld_dl = DataLoader(\n",
    "    sub_ds,\n",
    "    batch_size=CFG.bs,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.nw,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "vld_dl = DataLoader(\n",
    "    sub_ds,\n",
    "    batch_size=CFG.bs,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.nw,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "res = predict_tta(vld_dl, custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60af423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['target'] = torch.cat(res).view(-1).numpy()\n",
    "sub['id'] = sub['id'].apply(lambda x: x.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99c986c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('EXP_201_BASELINE_CASHE_V6_MIXER.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b848b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub['target'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36732d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30afe8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80698dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e9ff0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = list(Path('EXP_200_BASELINE_CASHE_V6').glob(\"*.pth\"))\n",
    "fns = [\"EXP_200_BASELINE_CASHE_V6/EXP_200_BASELINE_CASHE_V6_convmixer_1536_20_0_13.pth\", \n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6832ec7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______\n",
      "EXP_200_BASELINE_CASHE_V6/EXP_200_BASELINE_CASHE_V6_convmixer_1536_20_0_13.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▊                                                                           | 18/250 [00:19<03:43,  1.04it/s]/opt/conda/lib/python3.8/site-packages/numpy/core/_methods.py:246: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      " 10%|████████▍                                                                        | 26/250 [00:27<03:36,  1.03it/s]/opt/conda/lib/python3.8/site-packages/numpy/core/_methods.py:235: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      " 23%|██████████████████▍                                                              | 57/250 [00:57<03:10,  1.01it/s]/opt/conda/lib/python3.8/site-packages/numpy/core/_methods.py:246: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      " 42%|█████████████████████████████████▎                                              | 104/250 [01:44<02:25,  1.01it/s]/opt/conda/lib/python3.8/site-packages/numpy/core/_methods.py:246: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      " 43%|██████████████████████████████████▏                                             | 107/250 [01:47<02:22,  1.01it/s]/tmp/ipykernel_3277514/2093617167.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  data[1] = data[1] / data[1].mean()\n",
      " 58%|██████████████████████████████████████████████                                  | 144/250 [02:24<01:45,  1.01it/s]/tmp/ipykernel_3277514/2093617167.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  data[0] = data[0] / data[0].mean()\n",
      " 87%|█████████████████████████████████████████████████████████████████████▊          | 218/250 [03:37<00:31,  1.01it/s]/tmp/ipykernel_3277514/2093617167.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  data[1] = data[1] / data[1].mean()\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 250/250 [04:08<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "#predict train\n",
    "\n",
    "for mw in fns:\n",
    "    print('_______')\n",
    "    print(mw)\n",
    "    df_eval = pd.read_csv('../../val/val.csv')\n",
    "    df_eval.id = df_eval.id.apply(lambda x: Path(f\"../../val/v18val/{x}.pickle\"))\n",
    "    df_eval.columns = ['id', 'base_id', 'snr', 'target', 'f0', 'F1', 'F2', 'Alpha',\n",
    "           'Delta', 'cosi', 'psi', 'phi', 'path', 'freq', 'nonstationary',\n",
    "           'artifact']\n",
    "    sub_ds = ValLoaderPickle(df_eval)\n",
    "    vld_dl = DataLoader(\n",
    "        sub_ds,\n",
    "        batch_size=CFG.bs,\n",
    "        shuffle=False,\n",
    "        num_workers=CFG.nw,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    custom_model = create_model(\n",
    "                        CFG.model_name,\n",
    "                        pretrained=True,\n",
    "                        num_classes=1,\n",
    "                        in_chans=2,\n",
    "                    )\n",
    "\n",
    "    custom_model.load_state_dict(torch.load(mw))\n",
    "    custom_model.cuda();\n",
    "    custom_model.eval();\n",
    "\n",
    "    res = predict_tta(vld_dl, custom_model)\n",
    "    df_eval['pred'] = torch.cat(res).view(-1).numpy()\n",
    "    break\n",
    "    df_eval['snr'] = df_eval['snr'].replace(1000, 0)\n",
    "    df_eval = df_eval.dropna(subset='pred')\n",
    "    dict_res = generate_report(df_eval)\n",
    "    dict_res_400_500 = generate_report(df_eval.query('freq>400 and freq<500'))\n",
    "    dict_res_300_400 = generate_report(df_eval.query('freq>300 and freq<400'))\n",
    "    dict_res_200_300 = generate_report(df_eval.query('freq>200 and freq<300'))\n",
    "    dict_res_50_200 = generate_report(df_eval.query('freq>50 and freq<200'))\n",
    "    print('___all___')\n",
    "    print(dict_res)\n",
    "    print('freq_400_500:')\n",
    "    print(dict_res_400_500)\n",
    "    print('freq_300_400:')\n",
    "    print(dict_res_300_400)\n",
    "    print('freq_200_300:')\n",
    "    print(dict_res_200_300)\n",
    "    print('freq_50_200:')\n",
    "    print(dict_res_50_200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c8cc37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.id = df_eval.id.apply(lambda x: x.stem)\n",
    "df_eval.to_csv('EXP_200_BASELINE_CASHE_V6_MIXER_EVAL_CORRECT.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.724\n",
    "0.7242\n",
    "\n",
    "#0.7251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51276728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7111302727653694"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(df_eval.dropna(subset='pred')['target'], df_eval.dropna(subset='pred')['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "289e023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinearity(x):\n",
    "    # swish\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        in_channels,\n",
    "        out_channels=None,\n",
    "        conv_shortcut=False,\n",
    "        dropout,\n",
    "        temb_channels=512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_conv_shortcut = conv_shortcut\n",
    "\n",
    "        self.norm1 = Normalize(in_channels)\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        if temb_channels > 0:\n",
    "            self.temb_proj = torch.nn.Linear(temb_channels, out_channels)\n",
    "        self.norm2 = Normalize(out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        if self.in_channels != self.out_channels:\n",
    "            if self.use_conv_shortcut:\n",
    "                self.conv_shortcut = torch.nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "                )\n",
    "            else:\n",
    "                self.nin_shortcut = torch.nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=1, padding=0\n",
    "                )\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        h = x\n",
    "        h = self.norm1(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        if temb is not None:\n",
    "            h = h + self.temb_proj(nonlinearity(temb))[:, :, None, None]\n",
    "\n",
    "        h = self.norm2(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        if self.in_channels != self.out_channels:\n",
    "            if self.use_conv_shortcut:\n",
    "                x = self.conv_shortcut(x)\n",
    "            else:\n",
    "                x = self.nin_shortcut(x)\n",
    "\n",
    "        return x + h\n",
    "\n",
    "\n",
    "def Normalize(in_channels, num_groups=32):\n",
    "    return torch.nn.GroupNorm(\n",
    "        num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True\n",
    "    )\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        ch,\n",
    "        ch_mult=(1, 2,),\n",
    "        num_res_blocks,\n",
    "        attn_resolutions,\n",
    "        dropout=0.0,\n",
    "        resamp_with_conv=True,\n",
    "        in_channels,\n",
    "        resolution,\n",
    "        z_channels,\n",
    "        double_z=True,\n",
    "        embed_in = 0,\n",
    "        use_linear_attn=False,\n",
    "        attn_type=\"linear\",\n",
    "        **ignore_kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if use_linear_attn:\n",
    "            attn_type = \"linear\"\n",
    "        self.ch = ch\n",
    "        self.temb_ch = embed_in\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.resolution = resolution\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # downsampling\n",
    "        self.conv_in = torch.nn.Conv2d(\n",
    "            in_channels, self.ch, kernel_size=(12, 12), stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        curr_res = resolution\n",
    "        in_ch_mult = (1,) + tuple(ch_mult)\n",
    "        self.in_ch_mult = in_ch_mult\n",
    "        self.down = nn.ModuleList()\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_in = ch * in_ch_mult[i_level]\n",
    "            block_out = ch * ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                block.append(\n",
    "                    ResnetBlock(\n",
    "                        in_channels=block_in,\n",
    "                        out_channels=block_out,\n",
    "                        temb_channels=self.temb_ch,\n",
    "                        dropout=dropout,\n",
    "                    )\n",
    "                )\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
    "            down = nn.Module()\n",
    "            down.block = block\n",
    "            down.attn = attn\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res // 2\n",
    "            self.down.append(down)\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(\n",
    "            in_channels=block_in,\n",
    "            out_channels=block_in,\n",
    "            temb_channels=self.temb_ch,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
    "        self.mid.block_2 = ResnetBlock(\n",
    "            in_channels=block_in,\n",
    "            out_channels=block_in,\n",
    "            temb_channels=self.temb_ch,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(\n",
    "            block_in,\n",
    "            2 * z_channels if double_z else z_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, temb=None):\n",
    "        # timestep embedding\n",
    "        temb = temb\n",
    "\n",
    "        # downsampling\n",
    "        hs = [self.conv_in(x)]\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
    "                if len(self.down[i_level].attn) > 0:\n",
    "                    h = self.down[i_level].attn[i_block](h)\n",
    "                hs.append(h)\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
    "\n",
    "        # middle\n",
    "        h = hs[-1]\n",
    "        h = self.mid.block_1(h, temb)\n",
    "        h = self.mid.attn_1(h)\n",
    "        h = self.mid.block_2(h, temb)\n",
    "\n",
    "        # end\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv_out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, with_conv):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "        if self.with_conv:\n",
    "            # no asymmetric padding in torch conv, must do it ourselves\n",
    "            self.conv = torch.nn.Conv2d(\n",
    "                in_channels, in_channels, kernel_size=3, stride=2, padding=0\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.with_conv:\n",
    "            pad = (0, 1, 0, 1)\n",
    "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
    "            x = self.conv(x)\n",
    "        else:\n",
    "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_attn(in_channels, attn_type=\"vanilla\"):\n",
    "    assert attn_type in [\"vanilla\", \"linear\", \"none\"], f\"attn_type {attn_type} unknown\"\n",
    "    print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n",
    "    if attn_type == \"vanilla\":\n",
    "        return AttnBlock(in_channels)\n",
    "    elif attn_type == \"none\":\n",
    "        return nn.Identity(in_channels)\n",
    "    else:\n",
    "        return LinAttnBlock(in_channels)\n",
    "\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = rearrange(\n",
    "            qkv, \"b (qkv heads c) h w -> qkv b heads c (h w)\", heads=self.heads, qkv=3\n",
    "        )\n",
    "        k = k.softmax(dim=-1)\n",
    "        context = torch.einsum(\"bhdn,bhen->bhde\", k, v)\n",
    "        out = torch.einsum(\"bhde,bhdn->bhen\", context, q)\n",
    "        out = rearrange(\n",
    "            out, \"b heads c (h w) -> b (heads c) h w\", heads=self.heads, h=h, w=w\n",
    "        )\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.q = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.k = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.v = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.proj_out = torch.nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        b, c, h, w = q.shape\n",
    "        q = q.reshape(b, c, h * w)\n",
    "        q = q.permute(0, 2, 1)  # b,hw,c\n",
    "        k = k.reshape(b, c, h * w)  # b,c,hw\n",
    "        w_ = torch.bmm(q, k)  # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
    "        w_ = w_ * (int(c) ** (-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "\n",
    "        # attend to values\n",
    "        v = v.reshape(b, c, h * w)\n",
    "        w_ = w_.permute(0, 2, 1)  # b,hw,hw (first hw of k, second of q)\n",
    "        h_ = torch.bmm(v, w_)  # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
    "        h_ = h_.reshape(b, c, h, w)\n",
    "\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x + h_\n",
    "\n",
    "\n",
    "class LinAttnBlock(LinearAttention):\n",
    "    \"\"\"to match AttnBlock usage\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88c0e7cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'linear' with 64 in_channels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 175, 67])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoder(\n",
    "    ch=32,\n",
    "    num_res_blocks=3,\n",
    "    attn_resolutions=[],\n",
    "    in_channels=2,\n",
    "    resolution=256,\n",
    "    z_channels=3,\n",
    "    double_z=False,\n",
    "    embed_in=412,\n",
    ")(torch.rand(1, 2, 360, 144), torch.rand(1, 412)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95222c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'linear' with 64 in_channels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (conv_in): Conv2d(2, 32, kernel_size=(12, 12), stride=(1, 1), padding=(1, 1))\n",
       "  (down): ModuleList(\n",
       "    (0): Module(\n",
       "      (block): ModuleList(\n",
       "        (0): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (temb_proj): Linear(in_features=412, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (temb_proj): Linear(in_features=412, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (temb_proj): Linear(in_features=412, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (attn): ModuleList()\n",
       "      (downsample): Downsample(\n",
       "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (block): ModuleList(\n",
       "        (0): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (temb_proj): Linear(in_features=412, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nin_shortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (temb_proj): Linear(in_features=412, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (temb_proj): Linear(in_features=412, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (attn): ModuleList()\n",
       "    )\n",
       "  )\n",
       "  (mid): Module(\n",
       "    (block_1): ResnetBlock(\n",
       "      (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (temb_proj): Linear(in_features=412, out_features=64, bias=True)\n",
       "      (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (attn_1): LinAttnBlock(\n",
       "      (to_qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (to_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (block_2): ResnetBlock(\n",
       "      (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (temb_proj): Linear(in_features=412, out_features=64, bias=True)\n",
       "      (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (norm_out): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "  (conv_out): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoder(\n",
    "    ch=32,\n",
    "    num_res_blocks=3,\n",
    "    attn_resolutions=[],\n",
    "    in_channels=2,\n",
    "    resolution=256,\n",
    "    z_channels=3,\n",
    "    double_z=False,\n",
    "    embed_in=412,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b81495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
