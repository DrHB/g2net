{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b905e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    ToTensor,\n",
    "    Lambda,\n",
    "    ToPILImage,\n",
    "    CenterCrop,\n",
    "    Resize,\n",
    ")\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from einops import repeat\n",
    "from transformers.optimization import (\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from fastai.vision.all import L, unsqueeze\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import (\n",
    "    LabelSmoothingCrossEntropy,\n",
    "    BinaryCrossEntropy,\n",
    "    SoftTargetCrossEntropy,\n",
    ")\n",
    "\n",
    "from einops import rearrange\n",
    "import copy\n",
    "import h5py\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421402e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "import torch.nn as nn\n",
    "from thop import profile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "torch.cuda.set_device(1)\n",
    "##---------- Basic Layers ----------\n",
    "def conv3x3(in_chn, out_chn, bias=True):\n",
    "    layer = nn.Conv2d(in_chn, out_chn, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size, bias=False, stride=1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        padding=(kernel_size // 2),\n",
    "        bias=bias,\n",
    "        stride=stride,\n",
    "    )\n",
    "\n",
    "\n",
    "def bili_resize(factor):\n",
    "    return nn.Upsample(scale_factor=factor, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "\n",
    "##---------- Basic Blocks ----------\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, downsample):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.block = SK_RDB(in_channels=in_size, growth_rate=out_size, num_layers=3)\n",
    "        if downsample:\n",
    "            self.downsample = PS_down(out_size, out_size, downscale=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        if self.downsample:\n",
    "            out_down = self.downsample(out)\n",
    "            return out_down, out\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        # self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2, bias=True)\n",
    "        self.up = PS_up(in_size, out_size, upscale=2)\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, False)\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        out = torch.cat([up, bridge], dim=1)\n",
    "        out = self.conv_block(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "##---------- Resizing Modules (Pixel(Un)Shuffle) ----------\n",
    "class PS_down(nn.Module):\n",
    "    def __init__(self, in_size, out_size, downscale):\n",
    "        super(PS_down, self).__init__()\n",
    "        self.UnPS = nn.PixelUnshuffle(downscale)\n",
    "        self.conv1 = nn.Conv2d((downscale**2) * in_size, out_size, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.UnPS(x)  # h/2, w/2, 4*c\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PS_up(nn.Module):\n",
    "    def __init__(self, in_size, out_size, upscale):\n",
    "        super(PS_up, self).__init__()\n",
    "\n",
    "        self.PS = nn.PixelShuffle(upscale)\n",
    "        self.conv1 = nn.Conv2d(in_size // (upscale**2), out_size, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.PS(x)  # h/2, w/2, 4*c\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "##---------- Selective Kernel Feature Fusion (SKFF) ----------\n",
    "class SKFF(nn.Module):\n",
    "    def __init__(self, in_channels, height=3, reduction=8, bias=False):\n",
    "        super(SKFF, self).__init__()\n",
    "\n",
    "        self.height = height\n",
    "        d = max(int(in_channels / reduction), 4)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_du = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, d, 1, padding=0, bias=bias), nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.fcs = nn.ModuleList([])\n",
    "        for i in range(self.height):\n",
    "            self.fcs.append(\n",
    "                nn.Conv2d(d, in_channels, kernel_size=1, stride=1, bias=bias)\n",
    "            )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, inp_feats):\n",
    "        batch_size, n_feats, H, W = inp_feats[1].shape\n",
    "\n",
    "        inp_feats = torch.cat(inp_feats, dim=1)\n",
    "        inp_feats = inp_feats.view(\n",
    "            batch_size, self.height, n_feats, inp_feats.shape[2], inp_feats.shape[3]\n",
    "        )\n",
    "\n",
    "        feats_U = torch.sum(inp_feats, dim=1)\n",
    "        feats_S = self.avg_pool(feats_U)\n",
    "        feats_Z = self.conv_du(feats_S)\n",
    "\n",
    "        attention_vectors = [fc(feats_Z) for fc in self.fcs]\n",
    "        attention_vectors = torch.cat(attention_vectors, dim=1)\n",
    "        attention_vectors = attention_vectors.view(\n",
    "            batch_size, self.height, n_feats, 1, 1\n",
    "        )\n",
    "\n",
    "        attention_vectors = self.softmax(attention_vectors)\n",
    "        feats_V = torch.sum(inp_feats * attention_vectors, dim=1)\n",
    "\n",
    "        return feats_V\n",
    "\n",
    "\n",
    "##---------- Dense Block ----------\n",
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, I):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=3 // 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sk = SKFF(out_channels, height=2, reduction=8, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.relu(self.conv(x))\n",
    "        # output = torch.cat([x, x1], 1) # -> RDB\n",
    "        output = self.sk((x, x1))\n",
    "        return output\n",
    "\n",
    "\n",
    "##---------- Selective Kernel Residual Dense Block (SK-RDB) ----------\n",
    "class SK_RDB(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_layers):\n",
    "        super(SK_RDB, self).__init__()\n",
    "        self.identity = nn.Conv2d(in_channels, growth_rate, 1, 1, 0)\n",
    "        self.layers = nn.Sequential(\n",
    "            *[DenseLayer(in_channels, in_channels, I=i) for i in range(num_layers)]\n",
    "        )\n",
    "        self.lff = nn.Conv2d(in_channels, growth_rate, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.identity(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.lff(x)\n",
    "        return res + x\n",
    "\n",
    "\n",
    "##---------- testNet ----------\n",
    "class SRMNet(nn.Module):\n",
    "    def __init__(self, in_chn=3, wf=96, depth=4):\n",
    "        super(SRMNet, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.down_path = nn.ModuleList()\n",
    "        self.bili_down = bili_resize(0.5)\n",
    "        self.conv_01 = nn.Conv2d(in_chn, wf, 3, 1, 1)\n",
    "\n",
    "        # encoder of UNet\n",
    "        prev_channels = 0\n",
    "        for i in range(depth):  # 0,1,2,3\n",
    "            downsample = True if (i + 1) < depth else False\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels + wf, (2**i) * wf, downsample)\n",
    "            )\n",
    "            prev_channels = (2**i) * wf\n",
    "\n",
    "        # decoder of UNet\n",
    "        self.up_path = nn.ModuleList()\n",
    "        self.skip_conv = nn.ModuleList()\n",
    "        self.conv_up = nn.ModuleList()\n",
    "        self.bottom_conv = nn.Conv2d(prev_channels, wf, 3, 1, 1)\n",
    "        self.bottom_up = bili_resize(2 ** (depth - 1))\n",
    "\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(UNetUpBlock(prev_channels, (2**i) * wf))\n",
    "            self.skip_conv.append(nn.Conv2d((2**i) * wf, (2**i) * wf, 3, 1, 1))\n",
    "            self.conv_up.append(\n",
    "                nn.Sequential(\n",
    "                    *[nn.Conv2d((2**i) * wf, wf, 3, 1, 1), bili_resize(2**i)]\n",
    "                )\n",
    "            )\n",
    "            prev_channels = (2**i) * wf\n",
    "\n",
    "        self.final_ff = SKFF(in_channels=wf, height=depth)\n",
    "        self.last = conv3x3(prev_channels, in_chn, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        img = x\n",
    "        scale_img = img\n",
    "\n",
    "        ##### shallow conv #####\n",
    "        x1 = self.conv_01(img)\n",
    "        encs = []\n",
    "        ######## UNet ########\n",
    "        # Down-path (Encoder)\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            if i == 0:\n",
    "                x1, x1_up = down(x1)\n",
    "                encs.append(x1_up)\n",
    "            elif (i + 1) < self.depth:\n",
    "                scale_img = self.bili_down(scale_img)\n",
    "                left_bar = self.conv_01(scale_img)\n",
    "                x1 = torch.cat([x1, left_bar], dim=1)\n",
    "                x1, x1_up = down(x1)\n",
    "                encs.append(x1_up)\n",
    "            else:\n",
    "                scale_img = self.bili_down(scale_img)\n",
    "                left_bar = self.conv_01(scale_img)\n",
    "                x1 = torch.cat([x1, left_bar], dim=1)\n",
    "                x1 = down(x1)\n",
    "\n",
    "        # Up-path (Decoder)\n",
    "        ms_result = [self.bottom_up(self.bottom_conv(x1))]\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x1 = up(x1, self.skip_conv[i](encs[-i - 1]))\n",
    "            ms_result.append(self.conv_up[i](x1))\n",
    "        # Multi-scale selective feature fusion\n",
    "        msff_result = self.final_ff(ms_result)\n",
    "\n",
    "        ##### Reconstruct #####\n",
    "        out_1 = self.last(msff_result) + img\n",
    "\n",
    "        return out_1\n",
    "    \n",
    "    \n",
    "def normalize(x, pmin=3, pmax=97, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n",
    "    \"\"\"Percentile-based image normalization.\"\"\"\n",
    "\n",
    "    mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n",
    "    ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n",
    "    return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n",
    "\n",
    "\n",
    "def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):\n",
    "    if dtype is not None:\n",
    "        x   = x.astype(dtype,copy=False)\n",
    "        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n",
    "        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n",
    "        eps = dtype(eps)\n",
    "\n",
    "    try:\n",
    "        import numexpr\n",
    "        x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n",
    "    except ImportError:\n",
    "        x =                   (x - mi) / ( ma - mi + eps )\n",
    "\n",
    "    if clip:\n",
    "        x = np.clip(x,0,1)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def customplot(x, y, k=None):\n",
    "    if k:\n",
    "        plt.imshow(torch.concat([x.mean(0), y.mean(0), k.mean(0)], 1))\n",
    "    else:\n",
    "        plt.imshow(torch.concat([x.mean(0), y.mean(0)], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0759e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataV0Valid():\n",
    "    \"\"\"\n",
    "    dataset = Dataset(data_type, df)\n",
    "\n",
    "    img, y = dataset[i]\n",
    "      img (np.float32): 2 x 360 x 128\n",
    "      y (np.float32): label 0 or 1\n",
    "    \"\"\"\n",
    "    def __init__(self, df, freq_tfms=False):\n",
    "        self.df = df\n",
    "        self.freq_tfms = freq_tfms\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        i (int): get ith data\n",
    "        \"\"\"\n",
    "        r = self.df.iloc[i]\n",
    "        y = np.float32(r.target)\n",
    "        filename=r.id\n",
    "        file_id = Path(r.id).stem\n",
    "        img = np.empty((2, 360, 128), dtype=np.float32)\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            g = f[file_id]\n",
    "\n",
    "            for ch, s in enumerate(['H1', 'L1']):\n",
    "                a = g[s]['SFTs'][:, :4096] * 1e22  # Fourier coefficient complex64\n",
    "                p = a.real**2 + a.imag**2  # power\n",
    "                p = np.mean(p.reshape(360, 128, 32), axis=2)  # compress 4096 -> 128\n",
    "                img[ch] = normalize(p, clip=True)\n",
    "        return torch.tensor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cadcf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model =  SRMNet(in_chn=2)\n",
    "custom_model.load_state_dict(torch.load('EXP_40_01_NV_TEST_V3/EXP_40_01_NV_TEST_V3_SRG_REAL_NOISE_TO_VOID_TEST_V3_DATA_V31_9.pth'))\n",
    "custom_model.cuda();\n",
    "custom_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a8b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../data/SPLITS/V_20/val_df.csv').query('snr>0 and snr <40')\n",
    "#submit = submit.set_index('id').drop(pos + maybe + maybe_Extreme + no).reset_index()\n",
    "#submit.columns = [\"fn\", \"target\"]\n",
    "#submit[\"fn\"] = submit[\"fn\"].apply(lambda x: Path(\"../data/test\") / f\"{x}.hdf5\")\n",
    "#submit.columns = [\"id\", \"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b73b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_ds = DataV0Valid(submit)\n",
    "tst_ds[0][1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e35c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k in range(0, 1000):\n",
    "    img = tst_ds[k].unsqueeze(0)\n",
    "    print(f\"id: {tst_ds.df.iloc[k].id.split('/')[-1]}, snr: {tst_ds.df.iloc[k].snr}\")\n",
    "    with torch.no_grad():\n",
    "        out = custom_model(img.cuda()).detach().cpu()\n",
    "        \n",
    "        fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
    "        axs[0].set_title('H1/L1 mean')\n",
    "        axs[1].set_title('H1')\n",
    "        axs[2].set_title('L1')\n",
    "\n",
    "        axs[0] = axs[0].imshow(img[0].mean(0))\n",
    "        axs[1] = axs[1].imshow(img[0][0])\n",
    "        axs[2] = axs[2].imshow(img[0][1])\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
    "        axs[0].set_title('denoise: H1/L1 mean')\n",
    "        axs[1].set_title('denoise: H1')\n",
    "        axs[2].set_title('denoise: L1')\n",
    "\n",
    "        axs[0] = axs[0].imshow(out[0].mean(0))\n",
    "        axs[1] = axs[1].imshow(out[0][0])\n",
    "        axs[2] = axs[2].imshow(out[0][1])\n",
    "\n",
    "        print('________')\n",
    "\n",
    "        plt.pause(0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa431c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94776cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c7e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26463462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
